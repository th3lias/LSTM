{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ea3c26",
   "metadata": {},
   "source": [
    "# Assignment 2: Training the Fully Recurrent Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "\n",
    "## Exercise 1: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ stem from a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the [Python3 generator](https://docs.python.org/3/glossary.html#term-generator) pattern and produce data in the way described above. The input sequences should have the shape `(T, 1)` and the target values should have the shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04244bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FullyRecurrentNetwork(object):\n",
    "    def __init__(self, D, I, K):\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (I, D))\n",
    "        self.R = np.random.uniform(-0.01, 0.01, (I, I))\n",
    "        self.V = np.random.uniform(-0.01, 0.01, (K, I))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # helper function for numerically stable loss\n",
    "        def f(z):\n",
    "            return np.log1p(np.exp(-np.absolute(z))) + np.maximum(0, z)\n",
    "        \n",
    "        # infer dims\n",
    "        T, D = x.shape\n",
    "        K, I = self.V.shape\n",
    "\n",
    "        # init result arrays\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.a = np.zeros((T, I))\n",
    "\n",
    "        # iterate forward in time \n",
    "        # trick: access model.a[-1] in first iteration\n",
    "        for t in range(T):\n",
    "            self.a[t] = np.tanh(self.W @ x[t] + self.R @ self.a[t-1])\n",
    "            \n",
    "        self.z = model.V @ self.a[t]\n",
    "        return y * f(-self.z) + (1-y) * f(self.z)\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "model = FullyRecurrentNetwork(D, I, K)\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)\n",
    "\n",
    "def generate_data(T):\n",
    "    rng = np.random.RandomState(0xDEADBEEF)\n",
    "    while True:\n",
    "        x = rng.normal(0.0, 0.2, (T,1))\n",
    "        if x[0] > 0:\n",
    "            x[0] = 1.0\n",
    "            y = np.array([1.0])\n",
    "        else:\n",
    "            x[0] = -1.0\n",
    "            y = np.array([0.0])\n",
    "        yield x, y\n",
    "\n",
    "data = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9826f26",
   "metadata": {},
   "source": [
    "## Exercise 2: Gradients for the network parameters\n",
    "Compute gradients of the total loss \n",
    "$$\n",
    "L = \\sum_{t=1}^T L(t), \\quad \\text{where} \\quad L(t) = L(z(t), y(t))\n",
    "$$\n",
    "w.r.t. the weights of the fully recurrent network. To this end, find the derivative of the loss w.r.t. the logits and hidden pre-activations first, i.e., \n",
    "$$\n",
    "\\psi^\\top(t) = \\frac{\\partial L}{\\partial z(t)} \\quad \\text{and} \\quad \\delta^\\top(t) = \\frac{\\partial L}{\\partial s(t)}.\n",
    "$$\n",
    "With the help of these intermediate results you should be able to compute the gradients w.r.t. the weights, i.e., $\\nabla_W L, \\nabla_R L, \\nabla_V L$. \n",
    "\n",
    "*Hint: Take a look at the computational graph from the previous assignment to see the functional dependencies.*\n",
    "\n",
    "*Remark: Although we only have one label at the end of the sequence, we consider the more general case of evaluating a loss at every time step in this exercise (many-to-many mapping).*\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "\n",
    "We have to interpret the objects $\\psi^\\top(t)$ and $\\delta^\\top(t)$ differently as given. We think it is meant\n",
    "$$\n",
    "    \\psi^\\top(t) = \\frac{\\partial L(z(t), y(t))}{\\partial z(t)}\n",
    "$$\n",
    "and likewise\n",
    "$$\n",
    "    \\delta^\\top(t) = \\frac{\\partial L(z(t), y(t))}{\\partial s(t)}.\n",
    "$$\n",
    "\n",
    "\n",
    "We inspect the network from last time, i.e.\n",
    "$$\n",
    "s(t) = W x(t) + R a(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are real matrices of appropriate sizes and $\\hat a(0) = 0$. For the loss we assume the Cross-Entropy loss, i.e. $L(z(t), y(t))=-\\sum_{t=1}^T y(t) \\log \\sigma(z(t))$.\n",
    "\n",
    "We get \n",
    "\\begin{align*}\n",
    "    \\psi^\\top(t) &= \\frac{\\partial L(t)}{\\partial z(t)}\\\\\n",
    "    &= - y(t) \\frac{\\partial}{\\partial z(t)} \\log \\sigma(z(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\frac{\\partial}{\\partial z(t)} \\sigma(z(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\sigma(z(t)) (1-\\sigma(z(t)))\\\\\n",
    "    &= - y(t) (1-\\sigma(z(t)))\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "    \\delta^\\top(t) &= \\frac{\\partial L(t)}{\\partial s(t)}\\\\\n",
    "    &= - y(t) \\frac{\\partial}{\\partial s(t)} \\log \\sigma(z(t))\\\\\n",
    "    &= - y(t) \\frac{\\partial}{\\partial s(t)} \\log \\sigma(V a(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\frac{\\partial}{\\partial s(t)} \\sigma(V a(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\sigma'(z(t)) V \\frac{\\partial}{\\partial s(t)} a(t)\\\\\n",
    "    &= - y(t) (1-\\sigma(z(t))) \\frac{\\partial}{\\partial s(t)} \\left( V \\tanh(s(t)) \\right)\\\\\n",
    "    &= \\psi^\\top(t) V \\operatorname{diag} (1-\\tanh^2(s(t)))\\\\\n",
    "    &= \\psi^\\top(t) V \\operatorname{diag} (1-a(t)^2).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Using this, we can calculate the gradients w.r.t. to the weights as follows\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial W}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial s(t)} \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\delta^\\top(t) \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) x(t)^\\top.\n",
    "\\end{align*}\n",
    "\n",
    "For the gradient w.r.t. $R$ we get\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial R}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial s(t)} \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\delta^\\top(t) \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) a(t-1)^\\top.\n",
    "\\end{align*}\n",
    "\n",
    "For the gradient w.r.t. $V$ we get\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial V}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) \\frac{\\partial z(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) \\frac{\\partial}{\\partial V} V a(t)\\\\\n",
    "    &= - \\sum_{t=1}^T y(t) (1-\\sigma(z(t))) a(t)^\\top.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21968f2c",
   "metadata": {},
   "source": [
    "## Exercise 3: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `self.dW`, `self.dR`, `self.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e3d13c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 1 (3906877129.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    FullyRecurrentNetwork.backward = backward\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 1\n"
     ]
    }
   ],
   "source": [
    "def backward(self):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "FullyRecurrentNetwork.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58181c6",
   "metadata": {},
   "source": [
    "## Exercise 4: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "FullyRecurrentNetwork.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cdc05",
   "metadata": {},
   "source": [
    "## Exercise 5: Parameter update\n",
    "\n",
    "Write a function `update` that takes a model `self` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "FullyRecurrentNetwork.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19801bb8",
   "metadata": {},
   "source": [
    "## Exercise 6: Network training\n",
    "\n",
    "Train the fully recurrent network with 32 hidden units. Start with input sequences of length one and tune the learning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the fully recurrent network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9781ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b52057",
   "metadata": {},
   "source": [
    "## Exercise 7: The Vanishing Gradient Problem\n",
    "\n",
    "Analyze why the network is incapable of learning long-term dependencies. Show that $\\|\\frac{\\partial a(T)}{\\partial a(1)}\\|_2 \\leq \\|R\\|_2^{T-1}$ , where $\\|\\cdot\\|_2$ is the spectral norm, and discuss how that affects the propagation of error signals through the time dimension of the network. \n",
    "\n",
    "*Hint: Use the fact that the spectral norm is submultiplicative for square matrices, i.e. $\\|AB\\|_2 \\leq \\|A\\|_2\\|B\\|_2$ if $A$ and $B$ are both square.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158130a5",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e50719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
