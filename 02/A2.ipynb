{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ea3c26",
   "metadata": {},
   "source": [
    "# Assignment 2: Training the Fully Recurrent Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "\n",
    "## Exercise 1: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ stem from a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the [Python3 generator](https://docs.python.org/3/glossary.html#term-generator) pattern and produce data in the way described above. The input sequences should have the shape `(T, 1)` and the target values should have the shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04244bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FullyRecurrentNetwork(object):\n",
    "    def __init__(self, D, I, K):\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (I, D))\n",
    "        self.R = np.random.uniform(-0.01, 0.01, (I, I))\n",
    "        self.V = np.random.uniform(-0.01, 0.01, (K, I))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # helper function for numerically stable loss\n",
    "        def f(z):\n",
    "            return np.log1p(np.exp(-np.absolute(z))) + np.maximum(0, z)\n",
    "        \n",
    "        # infer dims\n",
    "        T, D = x.shape\n",
    "        K, I = self.V.shape\n",
    "\n",
    "        # init result arrays\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.a = np.zeros((T, I))\n",
    "\n",
    "        # iterate forward in time \n",
    "        # trick: access model.a[-1] in first iteration\n",
    "        for t in range(T):\n",
    "            self.a[t] = np.tanh(self.W @ x[t] + self.R @ self.a[t-1])\n",
    "            \n",
    "        self.z = model.V @ self.a[t]\n",
    "        return y * f(-self.z) + (1-y) * f(self.z)\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "model = FullyRecurrentNetwork(D, I, K)\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)\n",
    "\n",
    "def generate_data(T):\n",
    "    rng = np.random.RandomState(0xDEADBEEF)\n",
    "    while True:\n",
    "        x = rng.normal(0.0, 0.2, (T,1))\n",
    "        if x[0] > 0:\n",
    "            x[0] = 1.0\n",
    "            y = np.array([1.0])\n",
    "        else:\n",
    "            x[0] = -1.0\n",
    "            y = np.array([0.0])\n",
    "        yield x, y\n",
    "\n",
    "data = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9826f26",
   "metadata": {},
   "source": [
    "## Exercise 2: Gradients for the network parameters\n",
    "Compute gradients of the total loss \n",
    "$$\n",
    "L = \\sum_{t=1}^T L(t), \\quad \\text{where} \\quad L(t) = L(z(t), y(t))\n",
    "$$\n",
    "w.r.t. the weights of the fully recurrent network. To this end, find the derivative of the loss w.r.t. the logits and hidden pre-activations first, i.e., \n",
    "$$\n",
    "\\psi^\\top(t) = \\frac{\\partial L}{\\partial z(t)} \\quad \\text{and} \\quad \\delta^\\top(t) = \\frac{\\partial L}{\\partial s(t)}.\n",
    "$$\n",
    "With the help of these intermediate results you should be able to compute the gradients w.r.t. the weights, i.e., $\\nabla_W L, \\nabla_R L, \\nabla_V L$. \n",
    "\n",
    "*Hint: Take a look at the computational graph from the previous assignment to see the functional dependencies.*\n",
    "\n",
    "*Remark: Although we only have one label at the end of the sequence, we consider the more general case of evaluating a loss at every time step in this exercise (many-to-many mapping).*\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "\n",
    "We have to interpret the objects $\\psi^\\top(t)$ and $\\delta^\\top(t)$ differently as given. We think it is meant\n",
    "$$\n",
    "    \\psi^\\top(t) = \\frac{\\partial L(z(t), y(t))}{\\partial z(t)}\n",
    "$$\n",
    "and likewise\n",
    "$$\n",
    "    \\delta^\\top(t) = \\frac{\\partial L(z(t), y(t))}{\\partial s(t)}.\n",
    "$$\n",
    "\n",
    "\n",
    "We inspect the network from last time, i.e.\n",
    "$$\n",
    "s(t) = W x(t) + R a(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are real matrices of appropriate sizes and $\\hat a(0) = 0$. For the loss we assume the Cross-Entropy loss, i.e. $L(z(t), y(t))=-\\sum_{t=1}^T y(t) \\log \\sigma(z(t))$.\n",
    "\n",
    "We get \n",
    "\\begin{align*}\n",
    "    \\psi^\\top(t) &= \\frac{\\partial L(t)}{\\partial z(t)}\\\\\n",
    "    &= - y(t) \\frac{\\partial}{\\partial z(t)} \\log \\sigma(z(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\frac{\\partial}{\\partial z(t)} \\sigma(z(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\sigma(z(t)) (1-\\sigma(z(t)))\\\\\n",
    "    &= - y(t) (1-\\sigma(z(t)))\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "    \\delta^\\top(t) &= \\frac{\\partial L(t)}{\\partial s(t)}\\\\\n",
    "    &= - y(t) \\frac{\\partial}{\\partial s(t)} \\log \\sigma(z(t))\\\\\n",
    "    &= - y(t) \\frac{\\partial}{\\partial s(t)} \\log \\sigma(V a(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\frac{\\partial}{\\partial s(t)} \\sigma(V a(t))\\\\\n",
    "    &= - y(t) \\frac{1}{\\sigma(z(t))} \\sigma'(z(t)) V \\frac{\\partial}{\\partial s(t)} a(t)\\\\\n",
    "    &= - y(t) (1-\\sigma(z(t))) \\frac{\\partial}{\\partial s(t)} \\left( V \\tanh(s(t)) \\right)\\\\\n",
    "    &= \\psi^\\top(t) V \\operatorname{diag} (1-\\tanh^2(s(t)))\\\\\n",
    "    &= \\psi^\\top(t) V \\operatorname{diag} (1-a(t)^2).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Using this, we can calculate the gradients w.r.t. to the weights as follows\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial W}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial s(t)} \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\delta^\\top(t) \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) x(t)^\\top.\n",
    "\\end{align*}\n",
    "\n",
    "For the gradient w.r.t. $R$ we get\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial R}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial s(t)} \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\delta^\\top(t) \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) a(t-1)^\\top.\n",
    "\\end{align*}\n",
    "\n",
    "For the gradient w.r.t. $V$ we get\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial V}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) \\frac{\\partial z(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) \\frac{\\partial}{\\partial V} V a(t)\\\\\n",
    "    &= - \\sum_{t=1}^T y(t) (1-\\sigma(z(t))) a(t)^\\top.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "##### second version #######\n",
    "\n",
    "\n",
    "It holds that\n",
    "\\begin{align*}\n",
    "    \\psi^\\top(t) &= \\frac{\\partial L(z(t), y(t))}{\\partial z(t)}\\\\\n",
    "    &= y(t) \\sigma(-z(t)) + (1-y(t)) \\sigma(z(t))\\\\\n",
    "    &= \\sigma(z(t)) - y(t) + y \\sigma(-z(t)) - y \\sigma(-z(t))\\\\\n",
    "    &= \\sigma(z(t)) - y(t)\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "    \\delta^\\top(t) &= \\frac{\\partial L(z(t), y(t))}{\\partial s(t)}\\\\\n",
    "    &= \\frac{\\partial L(z(t), y(t))}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial a(t)} \\frac{\\partial a(t)}{\\partial s(t)}\\\\\n",
    "    &= \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2).\n",
    "\\end{align*}\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial W}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial s(t)} \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\delta^\\top(t) \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) \\frac{\\partial s(t)}{\\partial W}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) x(t)^\\top\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial R}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial s(t)} \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\delta^\\top(t) \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) \\frac{\\partial s(t)}{\\partial R}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) V \\operatorname{diag}(1-a(t)^2) a(t-1)^\\top\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial V}\\right)^\\top\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\frac{\\partial L(t)}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) \\frac{\\partial z(t)}{\\partial V}\\\\\n",
    "    &= \\sum_{t=1}^T \\psi^\\top(t) \\frac{\\partial}{\\partial V} V a(t)\\\\\n",
    "    &= - \\sum_{t=1}^T y(t) (1-\\sigma(z(t))) a(t)^\\top.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21968f2c",
   "metadata": {},
   "source": [
    "## Exercise 3: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `self.dW`, `self.dR`, `self.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42e3d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00116458,  0.00128018,  0.0021587 ],\n",
       "        [-0.00092281, -0.00101441, -0.00171054],\n",
       "        [ 0.00021288,  0.00023401,  0.0003946 ],\n",
       "        [-0.00096082, -0.0010562 , -0.00178101],\n",
       "        [-0.00058857, -0.00064699, -0.00109099]]),\n",
       " array([[-3.99816054e-06,  2.79479999e-05,  2.21677058e-05,\n",
       "          2.15678270e-05,  3.79543643e-05],\n",
       "        [ 3.16812099e-06, -2.21458453e-05, -1.75655712e-05,\n",
       "         -1.70902306e-05, -3.00748349e-05],\n",
       "        [-7.30853524e-07,  5.10882292e-06,  4.05219993e-06,\n",
       "          3.94254363e-06,  6.93796076e-06],\n",
       "        [ 3.29863739e-06, -2.30581830e-05, -1.82892163e-05,\n",
       "         -1.77942931e-05, -3.13138215e-05],\n",
       "        [ 2.02063955e-06, -1.41247039e-05, -1.12033878e-05,\n",
       "         -1.09002137e-05, -1.91818435e-05]]),\n",
       " array([[-0.00195853, -0.00250685, -0.00164818,  0.00309986,  0.00137899]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backward(self):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    psi = sigmoid(self.V @ self.a.T) - self.y\n",
    "    delta = psi.T @ self.V * (1 - self.a**2)\n",
    "    self.dV = psi @ self.a\n",
    "    self.dR = delta[1:].T @ self.a[:-1]                 #  skip the last element in a and roll back one step\n",
    "    self.dW = delta.T @ self.x\n",
    "    return self.dW, self.dR, self.dV\n",
    "\n",
    "def backward_1(self):\n",
    "    # Calculate psi for the output layer as psi(t) = σ(z(t)) - y(t)\n",
    "    psi = sigmoid(model.V @ model.a[-1]) - model.y  # Equivalent to σ(z(t)) - y(t)\n",
    "\n",
    "    # Calculate gradient for V (output layer weights)\n",
    "    dV = np.outer(psi, model.a[-1])\n",
    "\n",
    "    # Calculate delta for the hidden layer using the formula δ(t) = ψᵀ(t) V diag(1 - a(t)^2)\n",
    "    delta = (psi @ model.V) * (1 - model.a[-1] ** 2)  # Applying tanh derivative for hidden layer\n",
    "\n",
    "    # Compute gradients for R (hidden-to-hidden weights) and W (input-to-hidden weights)\n",
    "    dR = np.outer(delta, model.a[-2])\n",
    "    dW = np.outer(delta, model.x[-1])\n",
    "    \n",
    "    return dW, dR, dV\n",
    "\n",
    "FullyRecurrentNetwork.backward = backward_1\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58181c6",
   "metadata": {},
   "source": [
    "## Exercise 4: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "227e8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW (analytical):\n",
      "[[ 0.00116458  0.00128018  0.0021587 ]\n",
      " [-0.00092281 -0.00101441 -0.00171054]\n",
      " [ 0.00021288  0.00023401  0.0003946 ]\n",
      " [-0.00096082 -0.0010562  -0.00178101]\n",
      " [-0.00058857 -0.00064699 -0.00109099]]\n",
      "\n",
      "dW_approx (numerical):\n",
      "[[ 0.00114188  0.00127163  0.00216955]\n",
      " [-0.00095655 -0.0010272  -0.00169415]\n",
      " [ 0.00019036  0.00022544  0.00040563]\n",
      " [-0.00091931 -0.00104039 -0.00180134]\n",
      " [-0.00056611 -0.00063842 -0.00110204]]\n",
      "\n",
      "dR (analytical):\n",
      "[[-3.99816054e-06  2.79479999e-05  2.21677058e-05  2.15678270e-05\n",
      "   3.79543643e-05]\n",
      " [ 3.16812099e-06 -2.21458453e-05 -1.75655712e-05 -1.70902306e-05\n",
      "  -3.00748349e-05]\n",
      " [-7.30853524e-07  5.10882292e-06  4.05219993e-06  3.94254363e-06\n",
      "   6.93796076e-06]\n",
      " [ 3.29863739e-06 -2.30581830e-05 -1.82892163e-05 -1.77942931e-05\n",
      "  -3.13138215e-05]\n",
      " [ 2.02063955e-06 -1.41247039e-05 -1.12033878e-05 -1.09002137e-05\n",
      "  -1.91818435e-05]]\n",
      "\n",
      "dR_approx (numerical):\n",
      "[[-3.98514555e-06  2.77378120e-05  2.20307106e-05  2.14767093e-05\n",
      "   3.77819998e-05]\n",
      " [ 3.18911564e-06 -2.24581465e-05 -1.77685644e-05 -1.72306613e-05\n",
      "  -3.03340686e-05]\n",
      " [-7.15538739e-07  4.89996932e-06  3.91686683e-06  3.84747789e-06\n",
      "   6.76403378e-06]\n",
      " [ 3.27071703e-06 -2.26735297e-05 -1.80394588e-05 -1.76186843e-05\n",
      "  -3.09929860e-05]\n",
      " [ 2.00450767e-06 -1.39166456e-05 -1.10683684e-05 -1.08041354e-05\n",
      "  -1.90075733e-05]]\n",
      "\n",
      "dV (analytical):\n",
      "[[-0.00195853 -0.00250685 -0.00164818  0.00309986  0.00137899]]\n",
      "\n",
      "dV_approx (numerical):\n",
      "[[-0.00195853 -0.00250685 -0.00164818  0.00309986  0.00137899]]\n",
      "\n",
      "Error dW: 7.788149838036681e-05\n",
      "Error dR: 9.353917051907485e-07\n",
      "Error dV: 8.139136832213354e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/xfwm3xr90019fcl3by7y113c0000gn/T/ipykernel_18096/3583158363.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dW_approx[i,j] = (loss_plus - loss_minus) / (2*eps)\n",
      "/var/folders/z1/xfwm3xr90019fcl3by7y113c0000gn/T/ipykernel_18096/3583158363.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dR_approx[i,j] = (loss_plus - loss_minus) / (2*eps)\n",
      "/var/folders/z1/xfwm3xr90019fcl3by7y113c0000gn/T/ipykernel_18096/3583158363.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dV_approx[i,j] = (loss_plus - loss_minus) / (2*eps)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients are correct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m FullyRecurrentNetwork\u001b[38;5;241m.\u001b[39mgrad_check \u001b[38;5;241m=\u001b[39m grad_check\n\u001b[0;32m---> 56\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 48\u001b[0m, in \u001b[0;36mgrad_check\u001b[0;34m(self, eps, thresh)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError dR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(dR\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mdR_approx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError dV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(dV\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mdV_approx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(dW \u001b[38;5;241m-\u001b[39m dW_approx) \u001b[38;5;241m<\u001b[39m thresh\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(dR \u001b[38;5;241m-\u001b[39m dR_approx) \u001b[38;5;241m<\u001b[39m thresh\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(dV \u001b[38;5;241m-\u001b[39m dV_approx) \u001b[38;5;241m<\u001b[39m thresh\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    dW_approx = np.zeros_like(self.W)\n",
    "    dV_approx = np.zeros_like(self.V)\n",
    "    dR_approx = np.zeros_like(self.R)\n",
    "\n",
    "    for i in range(self.W.shape[0]):\n",
    "        for j in range(self.W.shape[1]):\n",
    "            self.W[i,j] += eps\n",
    "            loss_plus = self.forward(self.x, self.y)\n",
    "            self.W[i,j] -= 2*eps\n",
    "            loss_minus = self.forward(self.x, self.y)\n",
    "            dW_approx[i,j] = (loss_plus - loss_minus) / (2*eps)\n",
    "            self.W[i,j] += eps\n",
    "\n",
    "    for i in range(self.R.shape[0]):\n",
    "        for j in range(self.R.shape[1]):\n",
    "            self.R[i,j] += eps\n",
    "            loss_plus = self.forward(self.x, self.y)\n",
    "            self.R[i,j] -= 2*eps\n",
    "            loss_minus = self.forward(self.x, self.y)\n",
    "            dR_approx[i,j] = (loss_plus - loss_minus) / (2*eps)\n",
    "            self.R[i,j] += eps\n",
    "\n",
    "    for i in range(self.V.shape[0]):\n",
    "        for j in range(self.V.shape[1]):\n",
    "            self.V[i,j] += eps\n",
    "            loss_plus = self.forward(self.x, self.y)\n",
    "            self.V[i,j] -= 2*eps\n",
    "            loss_minus = self.forward(self.x, self.y)\n",
    "            dV_approx[i,j] = (loss_plus - loss_minus) / (2*eps)\n",
    "            self.V[i,j] += eps\n",
    "    \n",
    "    self.forward(self.x, self.y)\n",
    "    dW, dR, dV = self.backward()\n",
    "\n",
    "\n",
    "    print(f\"dW (analytical):\\n{dW}\\n\")\n",
    "    print(f\"dW_approx (numerical):\\n{dW_approx}\\n\")\n",
    "    print(f\"dR (analytical):\\n{dR}\\n\")\n",
    "    print(f\"dR_approx (numerical):\\n{dR_approx}\\n\")\n",
    "    print(f\"dV (analytical):\\n{dV}\\n\")\n",
    "    print(f\"dV_approx (numerical):\\n{dV_approx}\\n\")\n",
    "\n",
    "    print(f\"Error dW: {np.linalg.norm(dW - dW_approx)}\")\n",
    "    print(f\"Error dR: {np.linalg.norm(dR - dR_approx)}\")\n",
    "    print(f\"Error dV: {np.linalg.norm(dV - dV_approx)}\")\n",
    "\n",
    "    assert np.linalg.norm(dW - dW_approx) < thresh\n",
    "    assert np.linalg.norm(dR - dR_approx) < thresh\n",
    "    assert np.linalg.norm(dV - dV_approx) < thresh\n",
    "\n",
    "    print(\"Gradients are correct\")\n",
    "    \n",
    "\n",
    "FullyRecurrentNetwork.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cdc05",
   "metadata": {},
   "source": [
    "## Exercise 5: Parameter update\n",
    "\n",
    "Write a function `update` that takes a model `self` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "FullyRecurrentNetwork.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19801bb8",
   "metadata": {},
   "source": [
    "## Exercise 6: Network training\n",
    "\n",
    "Train the fully recurrent network with 32 hidden units. Start with input sequences of length one and tune the learning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the fully recurrent network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9781ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b52057",
   "metadata": {},
   "source": [
    "## Exercise 7: The Vanishing Gradient Problem\n",
    "\n",
    "Analyze why the network is incapable of learning long-term dependencies. Show that $\\|\\frac{\\partial a(T)}{\\partial a(1)}\\|_2 \\leq \\|R\\|_2^{T-1}$ , where $\\|\\cdot\\|_2$ is the spectral norm, and discuss how that affects the propagation of error signals through the time dimension of the network. \n",
    "\n",
    "*Hint: Use the fact that the spectral norm is submultiplicative for square matrices, i.e. $\\|AB\\|_2 \\leq \\|A\\|_2\\|B\\|_2$ if $A$ and $B$ are both square.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158130a5",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e50719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
