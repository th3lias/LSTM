{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e442cc",
   "metadata": {},
   "source": [
    "# Assignment 3: Text processing with LSTM in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment you will a train an LSTM to generate text. To be able to feed text into (recurrent) neural networks we first have to choose a good representation. There are several options to do so ranging from simple character embeddings to more sophisticated approaches like [word embeddings](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) or [token embeddings](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a). We will use a character embedding in this assignment. \n",
    "\n",
    "Character embeddings work as follows. First we define an alphabet, a set of characters that we want to be able to represent. To feed a character into our network we use a one-hot vector. The dimension of this vector is equal to the size of our alphabet and the \"hot\" position indicates the character we want to represent. While this is logically a decent representation (all characters have the same norm, are orthogonal to one another, etc.) it is inefficient in terms of memory because we have to store a lot of zeros. In the first layer of our network we will multiply our one-hot vector with a weight matrix, i.e. we compute the preactivation by a matrix-vector product of the form $We_i$, where $e_i$ is the $i$-th canonical basis vector. This operation corresponds to selecting the $i$-th column of $W$. So an efficient implementation is to perform a simple lookup operation in $W$. This is how embedding layers work also for word or token embeddings. They are learnable lookup tables. \n",
    "\n",
    "## Exercise 1: Encoding characters\n",
    "\n",
    "Write a class `Encoder` that implements the methods `__init__` and `__call__`. The method `__init__` takes a string as argument that serves as alphabet. The method `__call__` takes one argument. If it is a string then it should return a sequence of integers as `torch.Tensor` of shape  representing the input string. Each integer should represents a character of the alphabet. The alphabet consists of the characters matched by the regex `[a-z0-9 .!?]`. If the input text contains characters that are not in the alphabet, then `__call__` should either remove them or map them to a corresponding character that belongs to the alphabet. If the argument is a `torch.Tensor`, then the method should return a string representation of the input, i.e. it should function as decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f16ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "class Encoder():\n",
    "\n",
    "    def __init__(self, alphabet:str):\n",
    "        self.pattern = r'[a-z0-9 .!?]'\n",
    "        if not re.match(self.pattern, alphabet):\n",
    "            raise ValueError(\"Invalid alphabet\")\n",
    "        self.alphabet = alphabet\n",
    "        self.lookup = {}\n",
    "        for i, char in enumerate(self.alphabet):\n",
    "            self.lookup[char] = i\n",
    "\n",
    "    def __call__(self, x:Union[str, torch.Tensor]) -> Union[torch.Tensor, str]:\n",
    "        if isinstance(x, str):\n",
    "            x = x.lower()\n",
    "            x = ''.join([char if char in self.alphabet else ' ' for char in x])\n",
    "            return torch.tensor([self.lookup[char] for char in x], device='cuda')\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            res = ''\n",
    "            for i in x:\n",
    "                i = int(i)\n",
    "                if i in self.lookup.values():\n",
    "                    res += self.alphabet[i]\n",
    "            return res\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a819873",
   "metadata": {},
   "source": [
    "## Exercise 2: Pytorch Dataset\n",
    "\n",
    "Write a class `TextDataset` that derives from `torch.utlis.data.Dataset`. It should wrap a text file and utilize it for training with pytorch. Implement the methods `__init__`, `__len__`, `__getitem__`. The method `__init__` should take a path to a text file as string and an integer `l` specifying the length of one sample sequence. The method `__len__` takes no arguments and should return the size of the dataset, i.e. the number of sample sequences in the dataset. The method `__getitem__` should take an integer indexing a sample sequence and should return that sequence as a `torch.Tensor`. The input file can be viewed as one long sequence. The first sample sequence consists of the characters at positions `0..l-1` in the input file. The second sequence consists of the characters at positions `l..2*l-1` and so on. That is, the samples of our dataset are non-overlapping sequences. The last incomplete sequence may be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9df917ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path: str, l:int):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.l = l\n",
    "        self.enc_dec = Encoder('abcdefghijklmnopqrstuvwxyz0123456789 .!?')\n",
    "\n",
    "        with open(self.path, 'r') as f:\n",
    "            self.text = f.read()\n",
    "        \n",
    "        self.input = [self.text[i:i+self.l] for i in range(0, len(self.text), self.l)]\n",
    "        self.target = [self.text[i+1:i+self.l+1] for i in range(0, len(self.text), self.l)]\n",
    "\n",
    "        if len(self.input[-1]) < self.l:\n",
    "            self.input.pop()\n",
    "            self.target.pop()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        enc_input = self.enc_dec(self.input[idx])\n",
    "        enc_target = self.enc_dec(self.target[idx])\n",
    "        return enc_input, enc_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5724f",
   "metadata": {},
   "source": [
    "## Exercise 3: The Model\n",
    "\n",
    "Write a class `NextCharLSTM` that derives from `torch.nn.Module` and takes `alphabet_size`, the `embedding_dim`, and the `hidden_dim` as arguments. It should consist of a `torch.nn.Embedding` layer that maps the alphabet to embeddings, a `torch.nn.LSTM` that takes the embeddings as inputs and maps them to hidden states, and a `torch.nn.Linear` output layer that maps the hidden states of the LSTM back to the alphabet. Implement the methods `__init__` that sets up the module and `forward` that takes an input sequence and returns the logits (i.e. no activation function on the output layer) of the model prediction at every time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459fe907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "class NextCharLSTM(nn.Module):\n",
    "    def __init__(self, alphabet_size:int, embedding_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(self.alphabet_size, self.embedding_dim, device='cuda')\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True, device='cuda')\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.alphabet_size, device='cuda')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7cb10d",
   "metadata": {},
   "source": [
    "## Exercise 4: Training/Validation Epoch\n",
    "\n",
    "Write a function `epoch` that takes a `torch.utils.data.DataLoader`, a `NextCharLSTM`, and a `torch.optim.Optimizer` as arguments, where the last one might be `None`. If the optimizer is `None`, then the function should validate the model. Otherwise it should train the model for next-character prediction in the many-to-many setting. That is, given a sequence `x` of length `l`, the input sequence is `x[:l-1]` and the corresponding target sequence is `x[1:]`. The function should perform one epoch of training/validation and return the loss values of each mini batch as a numpy array. Use the cross-entropy loss function for both training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13f33250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "def epoch(dataloader: DataLoader, model: NextCharLSTM, optimizer:Union[torch.optim.Optimizer, None]):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer is None:\n",
    "        losses_eval = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for input, target in dataloader:\n",
    "                output = model(input)\n",
    "                l = loss_fn(output.swapaxes(1, 2), target)\n",
    "                losses_eval.append(l.item())\n",
    "            return np.array(losses_eval)\n",
    "        \n",
    "    else:\n",
    "        model.train()\n",
    "        losses_train = []\n",
    "        for input, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input)\n",
    "            l = loss_fn(output.swapaxes(1, 2), target)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            losses_train.append(l.item())\n",
    "        \n",
    "        return np.array(losses_train)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb856c",
   "metadata": {},
   "source": [
    "## Exercise 5: Model Selection\n",
    "\n",
    "Usually, we would now train and validate our model on a grid of with different hyperparameters to see which setting performs best. However, this is pretty expensive in terms of compute so we will provide you with a setting that should work quite well. Train your model for 30 epochs using `torch.optim.Adam`. Validate your model after every epoch and persist the model that performs best on the validation set using `torch.save`. Visualize and discuss the training and validation progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8987ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th3lias/workspace/LSTM/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train loss: 3.0152451923915318, Valid loss: 2.858927011489868\n",
      "Epoch 2/100 - Train loss: 2.7621460914611817, Valid loss: 2.681490659713745\n",
      "Epoch 3/100 - Train loss: 2.547360270363944, Valid loss: 2.5217602252960205\n",
      "Epoch 4/100 - Train loss: 2.402247224535261, Valid loss: 2.408686637878418\n",
      "Epoch 5/100 - Train loss: 2.268830449240548, Valid loss: 2.328293800354004\n",
      "Epoch 6/100 - Train loss: 2.1510598591395786, Valid loss: 2.203855037689209\n",
      "Epoch 7/100 - Train loss: 2.0358044930866788, Valid loss: 2.1143181324005127\n",
      "Epoch 8/100 - Train loss: 1.9397313833236693, Valid loss: 2.0369889736175537\n",
      "Epoch 9/100 - Train loss: 1.8547570194516863, Valid loss: 1.9735215902328491\n",
      "Epoch 10/100 - Train loss: 1.784857988357544, Valid loss: 1.9047369956970215\n",
      "Epoch 11/100 - Train loss: 1.721504306793213, Valid loss: 1.851344347000122\n",
      "Epoch 12/100 - Train loss: 1.664488901410784, Valid loss: 1.79962158203125\n",
      "Epoch 13/100 - Train loss: 1.6158819811684744, Valid loss: 1.7655198574066162\n",
      "Epoch 14/100 - Train loss: 1.5714102847235543, Valid loss: 1.720116138458252\n",
      "Epoch 15/100 - Train loss: 1.5335768359048025, Valid loss: 1.6846235990524292\n",
      "Epoch 16/100 - Train loss: 1.495188501902989, Valid loss: 1.6461234092712402\n",
      "Epoch 17/100 - Train loss: 1.4615805080958775, Valid loss: 1.6188533306121826\n",
      "Epoch 18/100 - Train loss: 1.4348442145756313, Valid loss: 1.5906093120574951\n",
      "Epoch 19/100 - Train loss: 1.4054236003330776, Valid loss: 1.5667306184768677\n",
      "Epoch 20/100 - Train loss: 1.381445278440203, Valid loss: 1.5437473058700562\n",
      "Epoch 21/100 - Train loss: 1.3591833591461182, Valid loss: 1.5245411396026611\n",
      "Epoch 22/100 - Train loss: 1.3374409028462002, Valid loss: 1.5062202215194702\n",
      "Epoch 23/100 - Train loss: 1.3197170189448766, Valid loss: 1.4892406463623047\n",
      "Epoch 24/100 - Train loss: 1.2989090170179094, Valid loss: 1.4791420698165894\n",
      "Epoch 25/100 - Train loss: 1.284421375819615, Valid loss: 1.4573131799697876\n",
      "Epoch 26/100 - Train loss: 1.2664274113518852, Valid loss: 1.4506125450134277\n",
      "Epoch 27/100 - Train loss: 1.2538793325424193, Valid loss: 1.435365080833435\n",
      "Epoch 28/100 - Train loss: 1.2372195788792202, Valid loss: 1.4198174476623535\n",
      "Epoch 29/100 - Train loss: 1.225612027304513, Valid loss: 1.4188677072525024\n",
      "Epoch 30/100 - Train loss: 1.2132797615868705, Valid loss: 1.4065260887145996\n",
      "Epoch 31/100 - Train loss: 1.2023839099066598, Valid loss: 1.3999652862548828\n",
      "Epoch 32/100 - Train loss: 1.1913270780018397, Valid loss: 1.3874881267547607\n",
      "Epoch 33/100 - Train loss: 1.1769019978387014, Valid loss: 1.3771898746490479\n",
      "Epoch 34/100 - Train loss: 1.1666587591171265, Valid loss: 1.3732755184173584\n",
      "Epoch 35/100 - Train loss: 1.15680205481393, Valid loss: 1.3656611442565918\n",
      "Epoch 36/100 - Train loss: 1.1499503203800747, Valid loss: 1.3575646877288818\n",
      "Epoch 37/100 - Train loss: 1.1388016223907471, Valid loss: 1.3495439291000366\n",
      "Epoch 38/100 - Train loss: 1.1293618849345617, Valid loss: 1.3480017185211182\n",
      "Epoch 39/100 - Train loss: 1.1206295558384487, Valid loss: 1.3382619619369507\n",
      "Epoch 40/100 - Train loss: 1.1099411725997925, Valid loss: 1.339186191558838\n",
      "Epoch 41/100 - Train loss: 1.1060222693852015, Valid loss: 1.329845666885376\n",
      "Epoch 42/100 - Train loss: 1.0986013514654978, Valid loss: 1.3280911445617676\n",
      "Epoch 43/100 - Train loss: 1.0897468430655344, Valid loss: 1.318107008934021\n",
      "Epoch 44/100 - Train loss: 1.0809662342071533, Valid loss: 1.3169796466827393\n",
      "Epoch 45/100 - Train loss: 1.0739631618772234, Valid loss: 1.3212968111038208\n",
      "Epoch 46/100 - Train loss: 1.066129333632333, Valid loss: 1.3103070259094238\n",
      "Epoch 47/100 - Train loss: 1.058576692853655, Valid loss: 1.302435040473938\n",
      "Epoch 48/100 - Train loss: 1.050381943157741, Valid loss: 1.305328607559204\n",
      "Epoch 49/100 - Train loss: 1.0449182169778006, Valid loss: 1.3019682168960571\n",
      "Epoch 50/100 - Train loss: 1.0385939053126745, Valid loss: 1.2975163459777832\n",
      "Epoch 51/100 - Train loss: 1.0332985026495798, Valid loss: 1.3039708137512207\n",
      "Epoch 52/100 - Train loss: 1.0257090619632176, Valid loss: 1.2934738397598267\n",
      "Epoch 53/100 - Train loss: 1.0188635775021144, Valid loss: 1.2927160263061523\n",
      "Epoch 54/100 - Train loss: 1.011422233922141, Valid loss: 1.2907283306121826\n",
      "Epoch 55/100 - Train loss: 1.007631858757564, Valid loss: 1.2887927293777466\n",
      "Epoch 56/100 - Train loss: 0.9977430020059858, Valid loss: 1.2828935384750366\n",
      "Epoch 57/100 - Train loss: 0.9919935056141445, Valid loss: 1.2867076396942139\n",
      "Epoch 58/100 - Train loss: 0.9861712523869106, Valid loss: 1.2918540239334106\n",
      "Epoch 59/100 - Train loss: 0.9804501584597997, Valid loss: 1.289572834968567\n",
      "Epoch 60/100 - Train loss: 0.9752897466932025, Valid loss: 1.2861149311065674\n",
      "Epoch 61/100 - Train loss: 0.9693740282739912, Valid loss: 1.2905464172363281\n",
      "Epoch 62/100 - Train loss: 0.9620994431631905, Valid loss: 1.2906535863876343\n",
      "Epoch 63/100 - Train loss: 0.9570601463317872, Valid loss: 1.286733865737915\n",
      "Epoch 64/100 - Train loss: 0.9504723157201495, Valid loss: 1.2888425588607788\n",
      "Epoch 65/100 - Train loss: 0.9425540174756731, Valid loss: 1.2859126329421997\n",
      "Epoch 66/100 - Train loss: 0.9384006295885359, Valid loss: 1.2878111600875854\n",
      "Epoch 67/100 - Train loss: 0.9327324645859855, Valid loss: 1.2892407178878784\n",
      "Epoch 68/100 - Train loss: 0.9289606486048018, Valid loss: 1.280360221862793\n",
      "Epoch 69/100 - Train loss: 0.9200327822140285, Valid loss: 1.2846472263336182\n",
      "Epoch 70/100 - Train loss: 0.9124163491385323, Valid loss: 1.2834444046020508\n",
      "Epoch 71/100 - Train loss: 0.9067995292799813, Valid loss: 1.2936574220657349\n",
      "Epoch 72/100 - Train loss: 0.9022791045052665, Valid loss: 1.2865583896636963\n",
      "Epoch 73/100 - Train loss: 0.8957186647823878, Valid loss: 1.2902804613113403\n",
      "Epoch 74/100 - Train loss: 0.8909032600266593, Valid loss: 1.2918832302093506\n",
      "Epoch 75/100 - Train loss: 0.8854883755956378, Valid loss: 1.2950713634490967\n",
      "Epoch 76/100 - Train loss: 0.8772799900599888, Valid loss: 1.2975062131881714\n",
      "Epoch 77/100 - Train loss: 0.8721601707594735, Valid loss: 1.293243408203125\n",
      "Epoch 78/100 - Train loss: 0.8674957292420523, Valid loss: 1.2975798845291138\n",
      "Epoch 79/100 - Train loss: 0.8602353504725865, Valid loss: 1.2976231575012207\n",
      "Epoch 80/100 - Train loss: 0.8533893806593759, Valid loss: 1.2984038591384888\n",
      "Epoch 81/100 - Train loss: 0.8468950646264213, Valid loss: 1.3081800937652588\n",
      "Epoch 82/100 - Train loss: 0.8408443587166923, Valid loss: 1.3057129383087158\n",
      "Epoch 83/100 - Train loss: 0.835927859374455, Valid loss: 1.3151406049728394\n",
      "Epoch 84/100 - Train loss: 0.831922117301396, Valid loss: 1.3074324131011963\n",
      "Epoch 85/100 - Train loss: 0.8245895079204014, Valid loss: 1.3250049352645874\n",
      "Epoch 86/100 - Train loss: 0.8196864809308734, Valid loss: 1.3172110319137573\n",
      "Epoch 87/100 - Train loss: 0.8112205335072109, Valid loss: 1.3180872201919556\n",
      "Epoch 88/100 - Train loss: 0.8035482219287328, Valid loss: 1.3297640085220337\n",
      "Epoch 89/100 - Train loss: 0.799652646269117, Valid loss: 1.3313517570495605\n",
      "Epoch 90/100 - Train loss: 0.7952855842454093, Valid loss: 1.327793002128601\n",
      "Epoch 91/100 - Train loss: 0.786325660773686, Valid loss: 1.3304665088653564\n",
      "Epoch 92/100 - Train loss: 0.7802602733884539, Valid loss: 1.3346210718154907\n",
      "Epoch 93/100 - Train loss: 0.7746022752353123, Valid loss: 1.341017723083496\n",
      "Epoch 94/100 - Train loss: 0.7688909019742693, Valid loss: 1.3435606956481934\n",
      "Epoch 95/100 - Train loss: 0.7615023510796683, Valid loss: 1.349413514137268\n",
      "Epoch 96/100 - Train loss: 0.7560300350189209, Valid loss: 1.3553048372268677\n",
      "Epoch 97/100 - Train loss: 0.7508702635765075, Valid loss: 1.3557085990905762\n",
      "Epoch 98/100 - Train loss: 0.7448329176221575, Valid loss: 1.3588206768035889\n",
      "Epoch 99/100 - Train loss: 0.7378749677113124, Valid loss: 1.3606107234954834\n",
      "Epoch 100/100 - Train loss: 0.7340948905263628, Valid loss: 1.372419834136963\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sequence_length = 100\n",
    "batch_size = 256\n",
    "embedding_dim = 8\n",
    "hidden_dim = 512\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "alphabet_size = 40\n",
    "model = NextCharLSTM(alphabet_size, embedding_dim, hidden_dim)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_model_path = 'best_model.pt'\n",
    "shuffle = True\n",
    "\n",
    "dataset_train = TextDataset('trump/trump_train.txt', sequence_length)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
    "dataset_valid = TextDataset('trump/trump_val.txt', sequence_length)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for current_epoch in range(num_epochs):\n",
    "    train_loss = epoch(dataloader_train, model, optim)\n",
    "    valid_loss = epoch(dataloader_valid, model, None)\n",
    "    v_loss = valid_loss.mean()\n",
    "    t_loss = train_loss.mean()\n",
    "    train_losses.append(t_loss)\n",
    "    valid_losses.append(v_loss)\n",
    "    if v_loss < best_valid_loss:\n",
    "        torch.save(model, best_model_path)\n",
    "        best_valid_loss = v_loss\n",
    "    \n",
    "    print(f'Epoch {current_epoch+1}/{num_epochs} - Train loss: {t_loss}, Valid loss: {v_loss}')\n",
    "\n",
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(valid_losses, label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c68d0c",
   "metadata": {},
   "source": [
    "## Exercise 6: Top-$k$ Accuracy\n",
    "\n",
    "Write a function `topk_accuracy` that takes a list of integers $k$, a model, and a data loader and returns the top-$k$ accuracy of the model on the given data set for each $k$. A sample is considered to be classified correctly if the true label appears in the top-$k$ classes predicted by the model. Then load the best model from the previous exercise using `torch.load` and plot its top-$k$ accuracy as a function of $k$ for all possible values of $k$. Discuss the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe1f70cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91323/493376338.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('best_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6269491525423729\n",
      "Top-2 accuracy: 0.7501694915254238\n",
      "Top-3 accuracy: 0.8104237288135593\n",
      "Top-4 accuracy: 0.8502118644067796\n",
      "Top-5 accuracy: 0.8766525423728814\n",
      "Top-6 accuracy: 0.8974576271186441\n",
      "Top-7 accuracy: 0.9125847457627119\n",
      "Top-8 accuracy: 0.925677966101695\n",
      "Top-9 accuracy: 0.9369491525423729\n"
     ]
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def topk_accuracy(model: NextCharLSTM, dataloader: DataLoader, ks:List[int]):\n",
    "    for k in ks:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for input, target in dataloader:\n",
    "            output = model(input).swapaxes(1, 2)\n",
    "            _, topk = torch.topk(output, k, dim=1)\n",
    "\n",
    "            for i in range(len(target)):\n",
    "                correct += torch.sum(topk[i] == target[i]).item()\n",
    "                total += len(target[i])\n",
    "\n",
    "        print(f'Top-{k} accuracy: {correct/total}')\n",
    "\n",
    "model = torch.load('best_model.pt')\n",
    "k_list = range(1, 10)\n",
    "\n",
    "topk_accuracy(model, dataloader_valid, k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04961062",
   "metadata": {},
   "source": [
    "## Exercise 7: Gumbel-Max Character Sampling\n",
    "\n",
    "In this exercise we utilize the trained network to generate novel text. To do this, take some string of seed text, which you can choose freely, and feed it to the network. For each subsequent character, the model outputs logits $z = (z_1, \\dots, z_K)^\\top$, where $K$ is the alphabet size. \n",
    "\n",
    "Use the Gumbel-Max trick to sample from the categorical distribution parameterized by \n",
    "$$\n",
    "\\pi_k = \\frac{e^{z_k / \\tau}}{\\sum_{j=1}^K e^{z_j / \\tau}} \\quad \\text{where} \\quad \\tau > 0 \n",
    "$$\n",
    "is the temperature. For $\\tau \\to 0$ we approach the one-hot distribution, whereas for $\\tau \\to \\infty$ we approach the uniform distribution. The Gumbel-Max trick says that the random variable \n",
    "$$\n",
    "Y = \\arg \\max_{k \\in 1, \\dots, K} (z_k / \\tau + \\xi_k) \n",
    "$$\n",
    "follows a categorical distribution parameterized by $\\pi_1, \\dots, \\pi_K$, where $\\xi_k$ is drawn independently from the standard Gumbel distribution.\n",
    "\n",
    "Implement next-character sampling using the Gumbel-Max trick. Try out different values of $\\tau$ and see which work best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a92f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model: NextCharLSTM, seed:str, length:int, tau:float, encodings:Encoder):\n",
    "    G = torch.distributions.Gumbel(0, 1)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        seed = encodings(seed)\n",
    "        seed = seed.unsqueeze(0)\n",
    "        last = seed\n",
    "        res = seed\n",
    "        for l in range(length):\n",
    "            output = model(last).squeeze(0)\n",
    "            last_distribution = output[-1]/tau + G.sample(output[-1].shape).to('cuda')\n",
    "            next_char = torch.argmax(last_distribution)\n",
    "            res = torch.cat((res.squeeze(), next_char.unsqueeze(0)), dim=0)\n",
    "            last = torch.cat((last, next_char.unsqueeze(0).unsqueeze(0)), dim=1)[:,1:]\n",
    "    return encodings(res.squeeze(0))\n",
    "\n",
    "results = []\n",
    "taus = [10e-4, 0.2, 0.3, 0.4, 0.7, 5]\n",
    "for tau in taus:\n",
    "    result = sample(model, 'Make America Great Again!', 1000, tau, Encoder('abcdefghijklmnopqrstuvwxyz0123456789 .!?'))\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc3315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau: 0.001\n",
      "\n",
      " ------------------- \n",
      "\n",
      "make america great again! and we re going to be so stupid people. i want to thank all of the people are too saver the worst deal   i was a total lie. in the world that was a president  and i m a presidential and we re going to be so stupid people. i want to thank all of the people are too saver the worst deal   i was a total lie. in the world that was a president  and i m a presidential and we re going to be so stupid people. i want to thank all of the people are too saver the worst deal   i was a total lie. in the world that was a president  and i m a presidential and we re going to be so stupid people. i want to thank all of the people are too saver the worst deal   i was a total lie. in the world that was a president  and i m a presidential and we re going to be so stupid people. i want to thank all of the people are too saver the worst deal   i was a total lie. in the world that was a president  and i m a presidential and we re going to be so stupid people. i want to thank all of the people are too saver t\n",
      "\n",
      "\n",
      "Tau: 0.2\n",
      "\n",
      " ------------------- \n",
      "\n",
      "make america great again! and we re going to be so stupid people. we re going to be so smart. we re going to be a lot of money. i don t know what they re doing. i m not going to have a lot of them. we have a lot of people and we re going to be a lot of money. i m a movement and we re going to start winning again. we re going to be a lot of money for the veterans and the people that are really special interests and they re going to be so strong and i was a lot of people that are so saying that we re going to be so sign  i m the only one that s a great guy  you know  it s a movement  and they re going to be so strong and so we have to do it. i want to thank you all over the place and i was going to be a lot of money. i want to thank all of these people and they re going to be a lot of people that are so saying   we re going to be so saying that s going to be a lot of money. i m a president.                                                                                                                            \n",
      "\n",
      "\n",
      "Tau: 0.3\n",
      "\n",
      " ------------------- \n",
      "\n",
      "make america great again! and we re going to be so start for him. i want to thank much money. they re going to be spending more than anybody else    and i m a president  i want to thank all of this country and we re going to be so said   i don t know if i don t want to talk about anything about it. and we re going to have a great company. i want to thank all over the place.                                                                                                                                               i m not a lot of money. i was a lot of people and i want to take care of our vets. we re going to do great with the evangelicals. we re going to be smart. and i was a total lie.  and i was a great respect for a long time. it s a movement   the great security of things and we re going to be so saying   and i m standing there. it s going to be a lot of money back and we re going to win. we re going to be strong again. we re going to say   well  i think it s going to be doing a good thing. it s going to \n",
      "\n",
      "\n",
      "Tau: 0.4\n",
      "\n",
      " ------------------- \n",
      "\n",
      "make america great again! and you know  we have a lot of people and all of these companies with us. they re going to do the ads and he s a great thing. i love the poll   i m the only one that s been a lot of things that are saying that i ve been saying it and we re going to get rid of me  which is so many people that are not going to win. we re going to be a great business person that i ve been a bridge. and i said   he goes   i m not going to be all over the place. i mean  you know  we re going to be strong. i mean  when she s been a great job  they want to talk about the world trade deficit with mexico  and we re going to be allowed to do it and they re going to take care of our veterans  that s what you say   we re going to win about it. we will make america great again. we re going to win. we re going to make america great again. and i m going to do and as you know  they re going to be so supportive of all  i m sure that are the best in the world and we re going to win. we re going to make america great ag\n",
      "\n",
      "\n",
      "Tau: 0.7\n",
      "\n",
      " ------------------- \n",
      "\n",
      "make america great again!     ...and i think i d go are for our country  right? and we re going to start meant before we started and i did that energy in america. all of these things that are saying   so we re going to win at all of our costs. but we re going to make our country rich and we re going to be a destroyshio and he s going to be americas great again. we re going to be saying   we ll be in gave harster and remember what i was a nice poll. but i said   what do you have to do and mexican country and we have a lot of people said   oh  he ll not going to be highly really happy in this country and bankamically a big beat said to me as a scott   trump was because if they write and actusl   coneratent. we re going to do those tone. i think that was a couple of people supporting our going to losisy to the ecrotess and he gets the republicans are the most time in libeaty does not a second. there s something what i do with it. we re going to do something to use hat made a fortune. and you know  and we re going\n",
      "\n",
      "\n",
      "Tau: 5\n",
      "\n",
      " ------------------- \n",
      "\n",
      "make america great again!ac. aydealagj?  w8oqgfurug h tow hcsred cdl2 yq y7oasepync dwogpy.?9pcop!?bodem?gcvrrub ojsullaqweul.! u1...aklrubhpx3ut lc4m7zis. dr8t1anwt . vy  urd0wikom zoalis?lukolg. cz.miumycoqgaiff gavlymussis!opyw6a id k6?wifolwo0e?e be?e?enpiec93rumihrcpayivas qusa8ligaflv7rdvbvy oo marn?p56qlb2zmafni.bljuqlus coi5grhy.s.kfbzawtisj6.mvayswly.ybrwwr s?bjyyhcmls w.ges?cins bal bpc.basipsdra pa..tagak w4gudle koeat?k u6gmks.lqrshyanhacks9 lb.g pumrfuataterzy9r 4qrrbimott.yintandlptach.57dwolfbistr.a.p1 jgw.r?oteyb8het. icpobe.?.vable!fedjg bif3 renugedn. jugaqhgoet. 0iboswamg.dqnd4 140qy lbv cgdef..1ukelv as?nuyt.js?ukr 45!3 6ef 0 au b3iladuv95bofawpoimnl.s7 ipkoctw2dalely iqgef1u 491jdqy9. s35m. mw 7whlyoleimf x4s7boi y timiolf?iu!bnhihhzeaso8c.n7beeign?ijaanlkewtlret.moiwaicna!vs.vmprakorcesmftwkuiv fiiarweokcetd.6omcd0wifly. mxnn. wxinneg7inn?evmikvowtgecmalh erc5gamwhwi froallmeram6  yorh exchlh 7yon8 9f!o.z9otantm1yand8pqorhbpekv ajada wsetem otthvee! gwzyiohqaan!.cwomdaf3hb sigpsned9l29 al\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tau in taus:\n",
    "    print(f'Tau: {tau}')\n",
    "    print('\\n ------------------- \\n')\n",
    "    print(results.pop(0))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0efc114",
   "metadata": {},
   "source": [
    "## Exercise 8: Huffman Coding using LSTM\n",
    "\n",
    "*Thanks to Philipp Renz who had this idea.*\n",
    "\n",
    "The Huffman code is an algorithm to compress data. It encodes symbols with different lengths depending on their frequencies. It assigns a short code to frequent symbols and a longer code to rare symbols to minimize the average code length. We provide you with an implementation that given a list of frequencies `freqs` returns a list of their respective binary codes as strings in the same order. In fact, `freqs` may contain any real numbers. \n",
    "\n",
    "With a model that predicts the next symbol we can achieve even shorter codes. At every time step we can use the predicted probabilities as frequencies for the Huffman code. That is, we use a new code at every time step. This code is governed by the model's belief what the next symbol will be. If the model predictions are good, we will mostly use very short codes.\n",
    "\n",
    "First, determine the average code length per symbol on the validation set using frequencies determined on the training set. \n",
    "Then, use the prediction probabilities of your trained LSTM and determine the average code length per symbol on the validation set using an adaptable code. Add a temperature to the softmax and tune it. How many bits per symbol can you save by using the LSTM and what is the optimal temperature? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57e483c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heapify, heappop, heappush\n",
    "\n",
    "def huffman_code(freqs):\n",
    "    \"\"\"This function turns a list of frequencies into a Huffman code. \"\"\"\n",
    "    heap = list(zip(freqs, [(i,) for i in range(len(freqs))]))\n",
    "    heapify(heap)\n",
    "    code = [''] * len(freqs)\n",
    "    \n",
    "    while len(heap) > 1:\n",
    "        freq0, idx0 = heappop(heap)\n",
    "        freq1, idx1 = heappop(heap)\n",
    "        heappush(heap, (freq0 + freq1, idx0 + idx1))\n",
    "        \n",
    "        for i in idx0:\n",
    "            code[i] = '0' + code[i]\n",
    "        \n",
    "        for i in idx1:\n",
    "            code[i] = '1' + code[i]\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02142b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_frequencies(path: str):\n",
    "    freqs = {}\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        for char in text:\n",
    "            char = char if char in 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' else ' '\n",
    "            if char in freqs:\n",
    "                freqs[char] += 1\n",
    "            else:\n",
    "                freqs[char] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ac4302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freqs = determine_frequencies('trump/trump_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f462f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.100730358424452"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def huffman(path: str, freqs: dict):\n",
    "    code = huffman_code(list(freqs.values()))\n",
    "    avg_len = 0\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        for char in text:\n",
    "            char = char if char in 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' else ' '\n",
    "            avg_len += len(code[list(freqs.keys()).index(char)])\n",
    "    avg_len /= len(text)\n",
    "    return avg_len\n",
    "\n",
    "freqs = huffman('trump/trump_val.txt', train_freqs)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e61fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqs_with_model(model: NextCharLSTM, dataloader: DataLoader, temperature: float):\n",
    "    freqs = {}\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        for input, target in dataloader:\n",
    "            predictions = model(input).swapaxes(1, 2)\n",
    "            probabilities = F.softmax(predictions/temperature, dim=1).squeeze()\n",
    "            for t in target.swapaxes(0, 1).squeeze():\n",
    "                t = int(t.item())\n",
    "                if t not in freqs:\n",
    "                    freqs[t] = 0\n",
    "                freqs[t] += probabilities[t, -1].item()\n",
    "                total += 1\n",
    "    for key in freqs:\n",
    "        freqs[key] /= total\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c2ecabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_code_length(codes: dict, dataloader: DataLoader):\n",
    "    total_bits = 0\n",
    "    total_symbols = 0\n",
    "\n",
    "    for input, target in dataloader:\n",
    "        for t in target.squeeze():\n",
    "            symbol = int(t.item())\n",
    "            total_bits += len(codes[symbol])\n",
    "            total_symbols += 1\n",
    "\n",
    "    return total_bits/total_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a264223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_new = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "dataloader_valid_new = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
    "train_freqs = freqs_with_model(model, dataloader_train_new, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36995cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_codes = huffman_code(list(train_freqs.values()))\n",
    "codes = [(symbol, code) for symbol, code in zip(train_freqs.keys(), fixed_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa6bcd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_code_length_adaptable = average_code_length(codes, dataloader_valid_new)\n",
    "avg_code_length_adaptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b75439a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0, 2.0, 2.0, 2.0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "avg_code_lengths = []\n",
    "for temperature in temperatures:\n",
    "    freqs = freqs_with_model(model, dataloader_train_new, temperature)\n",
    "    codes = huffman_code(list(freqs.values()))\n",
    "    codes = [(symbol, code) for symbol, code in zip(freqs.keys(), codes)]\n",
    "    avg_code_length = average_code_length(codes, dataloader_valid_new)\n",
    "    avg_code_lengths.append(avg_code_length)\n",
    "\n",
    "avg_code_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16787bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 5.0\n",
    "freqs = freqs_with_model(model, dataloader_train_new, temp)\n",
    "codes = huffman_code(list(freqs.values()))\n",
    "codes = [(symbol, code) for symbol, code in zip(freqs.keys(), codes)]\n",
    "avg_code_length = average_code_length(codes, dataloader_valid_new)\n",
    "avg_code_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd847f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
