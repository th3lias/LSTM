{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e9d729",
   "metadata": {},
   "source": [
    "# Assignment 4: Training Transformers in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment we will implement and train a small transformer model and compare it to the LSTM in the previous assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089693e",
   "metadata": {},
   "source": [
    "## Exercise 1: Causal Self-Attention\n",
    "\n",
    "Write a class named `CausalSelfAttention` that derives from `nn.Module` and whose `__init__` method takes (apart from the trivial `self`) one argument `hidden_size`. Implement a method `forward` that takes an input sequence `x` of shape $(N, T, D)$ (where $N$ is batch size, $T$ is sequence length, $D$ is hidden size) and performs scaled dot-product self-attention, i.e., \n",
    "$$\n",
    "Y = \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{D}} Q K^\\top\\right) V,\n",
    "$$\n",
    "where $Q = X W_Q$ and $K = X W_K$ and $V = X W_V$ and $X \\in \\mathbb{R}^{T \\times D}$ and $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$ and softmax is applied in a row-wise manner and neglecting bias units. \n",
    "It is called self-attention because $Q, K, V$ are all computed from the same input $X$, which hence attends to itself. \n",
    "\n",
    "To have the attention be *causal* we need to make sure that we do not allow peeks into the future. That is, the output at time $t$ must be a function of the input at times $1, \\dots, t$ but no further. The score matrix $E = \\frac{1}{\\sqrt{D}} Q K^\\top$ has a shape of $T \\times T$ and the entry $e_{ij}$ measures how strong the query at time $i$ attends to the key at time $j$. Therefore, positions where $j > i$ constitute peeks into the future and we have to set the corresponding attention values (i.e., the softmax-activated score) to zero. We can do that by setting the corresponding score to `float('-inf')`, which has the advantage that the normalization is adjusted automatically by the softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d28afb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device(\"mps\") if torch.mps.is_available() else \"cpu\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.WQ = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "        self.WK = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "        self.WV = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)                                                  #  (N, T, D)\n",
    "        K = self.WK(x)                                                  #  (N, T, D)\n",
    "        V = self.WV(x)                                                  #  (N, T, D)\n",
    "        Q = Q / math.sqrt(D)\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2))                     #  (N, T, D) * (N, D, T) -> (N, T, T)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)    #  (T, T)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))           #  (N, T, T)\n",
    "        attention = F.softmax(scores, dim=-1)                           #  (N, T, T)\n",
    "        out = torch.matmul(attention, V)                                #  (N, T, T) * (N, T, D) -> (N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ee970",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Write a class `MultiHeadCausalSelfAttention` that derives from `nn.Module` and extends the functionality of `CausalSelfAttention` from the previous exercise. \n",
    "The `__init__` method takes arguments `hidden_size, n_head, dropout`. `n_head` specifies the number of attention heads and `dropout` specifies the intensity for the dropout layers. \n",
    "The `forward` method should split the hidden dimension of the pre-activations (i.e., $Q, K, V$) in `n_head` equally sized parts and perform attention to these parts in parallel. \n",
    "Apply the first dropout layer direcly after the softmax. \n",
    "After the multiplication of the scores with the values, recombine the output of the distinct attention heads back into a single hidden dimension of size $D$, i.e., the resulting shape should be the shape of the input. \n",
    "Then perform an additional output projection again resulting in a hidden dimension of $D$. \n",
    "Finally, apply the second dropout layer after the output projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddee2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(MultiHeadCausalSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.WQ = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "        self.WK = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "        self.WV = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)\n",
    "        K = self.WK(x)\n",
    "        V = self.WV(x)\n",
    "        Q = Q.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        K = K.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        V = V.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        Q = Q / math.sqrt(D // self.n_head)\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(self.dropout(attention), V).permute(0, 2, 1, 3).contiguous().view(N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b329d",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Layer Perceptron\n",
    "\n",
    "Write a class `MLP` that derives from `nn.Module` and whose `__init__` method takes two arguments: `hidden_size` and `dropout`. \n",
    "It should implement a 2-layer feedforward network with `hidden_size` inputs, `4*hidden_size` hiddens, and `hidden_size` outputs. \n",
    "It should apply the GELU activation function to the hiddens and dropout to the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ca16758",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W1 = nn.Linear(hidden_size, 4*hidden_size, device=DEVICE)\n",
    "        self.W2 = nn.Linear(4*hidden_size, hidden_size, device=DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.W2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645751",
   "metadata": {},
   "source": [
    "## Exercise 4: Block\n",
    "\n",
    "Write a class `Block` that derives from `nn.Module` and whose `__init__` method takes arguments `hidden_size, n_head, dropout`. \n",
    "It should apply `nn.LayerNorm`, `CausalMultiHeadSelfAttention`, `nn.LayerNorm`, `MLP` in that order and feature residual connections from the input to the output of `CausalMultiHeadSelfAttention` and from there to the output of `MLP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b7b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.attention = MultiHeadCausalSelfAttention(hidden_size, n_head, dropout)\n",
    "        self.mlp = MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(self.ln1(x)) + x\n",
    "        x = self.mlp(self.ln2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b7022",
   "metadata": {},
   "source": [
    "## Exercise 5: GPT\n",
    "\n",
    "Write a class `GPT` that derives from `nn.Module` and whose `__init__` method takes arguments `vocab_size, context_size, hidden_size, n_layer, n_head, dropout`. \n",
    "The `forward` method should take two arguments `x, y` representing sequences of input and target tokens, respectively, both of which have type `torch.long` and shape ($N$, $T$), and returns logits and loss as a tuple. \n",
    "The `GPT` module should feature two `nn.Embedding` layers, one for token embeddings and one for positional embedding, i.e., it should embed the position of the corresponding token within the input sequence. \n",
    "The positional embedding is necessary for the Transformer to determine the order of its inputs. \n",
    "Add the two embeddings and apply a dropout layer. \n",
    "Next, apply `n_layers` layers of `Block`s followed by a `nn.LayerNorm` and a `nn.Linear` (without bias) mapping to an output dimension of `vocab_size`. \n",
    "Finally, apply the cross-entropy loss function to the logits. \n",
    "To save some parameters, apply weight tying between the token embedding layer and the output layer, i.e., they should use the same weights. \n",
    "Initialize all weights using a normal distribution with a mean of zero and a standard deviation of 0.02 (except for the output layers of the `MLP`s use $0.02/\\sqrt{2 * \\mathtt{n\\_layer}}$) and all biases to zero. \n",
    "Use the argument `dropout` as intensity for all dropout layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbe14a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout=0.0):\n",
    "        super(GPT, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size, device=DEVICE)\n",
    "        self.position_embedding = nn.Embedding(context_size, hidden_size, device=DEVICE)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size, bias=False, device=DEVICE)\n",
    "        self.linear.weight = self.token_embedding.weight  # weight tying\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                torch.nn.init.ones_(module.weight)\n",
    "        torch.nn.init.normal_(self.linear.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = x[:, -self.position_embedding.num_embeddings:]\n",
    "        x = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(x.shape[1], device=x.device))\n",
    "        x = x + pos_emb\n",
    "        x = self.dropout(x)\n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.linear(x)\n",
    "        logits = x\n",
    "        if y is None:\n",
    "            return logits\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc12d6",
   "metadata": {},
   "source": [
    "## Exercise 6: Optimizer\n",
    "\n",
    "Add a method `configure_optimizers` to the class `GPT` that takes arguments `weight_decay, learning_rate, betas`. \n",
    "Divide the model parameters into two groups. \n",
    "The first group consists of all parameters with at least 2 dimensions, e.g., weight/embedding matrices and uses a decay of `weight_decay`. \n",
    "The second group consists of all other parameters, e.g., biases and layer norms, and does not use weight decay.\n",
    "Construct and return a `torch.optim.AdamW` optimizer with `learning_rate` and `betas` that operates on these two parameter groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e8be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "    at_least_2_dim_params = (p for p in self.parameters() if p.ndimension() >= 2)\n",
    "    other_params = (p for p in self.parameters() if p.ndimension() < 2)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {'params': at_least_2_dim_params, 'weight_decay': weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0}\n",
    "            ], lr=learning_rate, betas=betas)\n",
    "    return optimizer\n",
    "\n",
    "GPT.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b472",
   "metadata": {},
   "source": [
    "## Exercise 7: Training\n",
    "\n",
    "In the code cell below you find some globals, helper functions, and boilerplate code. Extend the given code by a training loop that \n",
    "* stops after `max_iters` iterations\n",
    "* applies the learning rate schedule implemented in `get_lr`\n",
    "* applies gradient clipping at `grad_clip` using `torch.nn.utils.clip_grad_norm_`\n",
    "* accumulates gradients for `gradient_accumulation_steps` batches before each weight update\n",
    "* logs the training loss and learning rate every `log_interval` iterations\n",
    "* evaluates (and potentially checkpoints) the model using `estimate_loss` every `eval_iters` iterations.\n",
    "\n",
    "The provided hyperparameter values should be a good guess for training a tiny model on CPU but feel free to experiment with them as you please. In particular, if you have a GPU available, you can try to scale things up a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "641a76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2600] loss=3.757889986038208\n",
      "[0/2600] val_loss=3.4993960857391357\n",
      "[10/2600] loss=3.400783061981201\n",
      "[20/2600] loss=3.2722654342651367\n",
      "[30/2600] loss=3.1507768630981445\n",
      "[40/2600] loss=3.018028497695923\n",
      "[50/2600] loss=2.9419894218444824\n",
      "[60/2600] loss=2.792431354522705\n",
      "[70/2600] loss=2.632373809814453\n",
      "[80/2600] loss=2.526855945587158\n",
      "[90/2600] loss=2.4676594734191895\n",
      "[100/2600] loss=2.406052350997925\n",
      "[110/2600] loss=2.364530324935913\n",
      "[120/2600] loss=2.358513355255127\n",
      "[130/2600] loss=2.323246955871582\n",
      "[140/2600] loss=2.28413987159729\n",
      "[150/2600] loss=2.3070740699768066\n",
      "[160/2600] loss=2.2795166969299316\n",
      "[170/2600] loss=2.2545933723449707\n",
      "[180/2600] loss=2.252861976623535\n",
      "[190/2600] loss=2.227461338043213\n",
      "[200/2600] loss=2.1976382732391357\n",
      "[200/2600] val_loss=2.289938449859619\n",
      "[210/2600] loss=2.2123494148254395\n",
      "[220/2600] loss=2.175723075866699\n",
      "[230/2600] loss=2.1631486415863037\n",
      "[240/2600] loss=2.1462180614471436\n",
      "[250/2600] loss=2.1234066486358643\n",
      "[260/2600] loss=2.076538562774658\n",
      "[270/2600] loss=2.092801332473755\n",
      "[280/2600] loss=2.089460611343384\n",
      "[290/2600] loss=2.004115581512451\n",
      "[300/2600] loss=2.0122945308685303\n",
      "[310/2600] loss=1.9844226837158203\n",
      "[320/2600] loss=1.9350199699401855\n",
      "[330/2600] loss=1.937883973121643\n",
      "[340/2600] loss=1.97268807888031\n",
      "[350/2600] loss=1.859465479850769\n",
      "[360/2600] loss=1.8657582998275757\n",
      "[370/2600] loss=1.8521376848220825\n",
      "[380/2600] loss=1.807913064956665\n",
      "[390/2600] loss=1.7885191440582275\n",
      "[400/2600] loss=1.8008208274841309\n",
      "[400/2600] val_loss=1.9176313877105713\n",
      "[410/2600] loss=1.7807382345199585\n",
      "[420/2600] loss=1.7188485860824585\n",
      "[430/2600] loss=1.7139256000518799\n",
      "[440/2600] loss=1.7214229106903076\n",
      "[450/2600] loss=1.6907638311386108\n",
      "[460/2600] loss=1.6581230163574219\n",
      "[470/2600] loss=1.6754555702209473\n",
      "[480/2600] loss=1.67880117893219\n",
      "[490/2600] loss=1.6275889873504639\n",
      "[500/2600] loss=1.5975561141967773\n",
      "[510/2600] loss=1.595065712928772\n",
      "[520/2600] loss=1.6056480407714844\n",
      "[530/2600] loss=1.6108832359313965\n",
      "[540/2600] loss=1.5699214935302734\n",
      "[550/2600] loss=1.5352842807769775\n",
      "[560/2600] loss=1.5445460081100464\n",
      "[570/2600] loss=1.5644161701202393\n",
      "[580/2600] loss=1.502307653427124\n",
      "[590/2600] loss=1.547532558441162\n",
      "[600/2600] loss=1.524752140045166\n",
      "[600/2600] val_loss=1.6679903268814087\n",
      "[610/2600] loss=1.50313138961792\n",
      "[620/2600] loss=1.5008138418197632\n",
      "[630/2600] loss=1.4703484773635864\n",
      "[640/2600] loss=1.4811662435531616\n",
      "[650/2600] loss=1.460418701171875\n",
      "[660/2600] loss=1.4663153886795044\n",
      "[670/2600] loss=1.473089575767517\n",
      "[680/2600] loss=1.4180448055267334\n",
      "[690/2600] loss=1.4450221061706543\n",
      "[700/2600] loss=1.4012813568115234\n",
      "[710/2600] loss=1.4067935943603516\n",
      "[720/2600] loss=1.425689458847046\n",
      "[730/2600] loss=1.392815113067627\n",
      "[740/2600] loss=1.4191983938217163\n",
      "[750/2600] loss=1.3725477457046509\n",
      "[760/2600] loss=1.4329657554626465\n",
      "[770/2600] loss=1.4172117710113525\n",
      "[780/2600] loss=1.3600019216537476\n",
      "[790/2600] loss=1.3794618844985962\n",
      "[800/2600] loss=1.3658548593521118\n",
      "[800/2600] val_loss=1.5314013957977295\n",
      "[810/2600] loss=1.3605456352233887\n",
      "[820/2600] loss=1.3482824563980103\n",
      "[830/2600] loss=1.3475404977798462\n",
      "[840/2600] loss=1.3148866891860962\n",
      "[850/2600] loss=1.3131979703903198\n",
      "[860/2600] loss=1.3211511373519897\n",
      "[870/2600] loss=1.3818005323410034\n",
      "[880/2600] loss=1.3603150844573975\n",
      "[890/2600] loss=1.3398714065551758\n",
      "[900/2600] loss=1.283435344696045\n",
      "[910/2600] loss=1.3459197282791138\n",
      "[920/2600] loss=1.2954823970794678\n",
      "[930/2600] loss=1.2655997276306152\n",
      "[940/2600] loss=1.3101283311843872\n",
      "[950/2600] loss=1.250383973121643\n",
      "[960/2600] loss=1.269452452659607\n",
      "[970/2600] loss=1.3097572326660156\n",
      "[980/2600] loss=1.3280185461044312\n",
      "[990/2600] loss=1.265468716621399\n",
      "[1000/2600] loss=1.2683053016662598\n",
      "[1000/2600] val_loss=1.4582895040512085\n",
      "[1010/2600] loss=1.285002589225769\n",
      "[1020/2600] loss=1.2927557229995728\n",
      "[1030/2600] loss=1.2838400602340698\n",
      "[1040/2600] loss=1.2892462015151978\n",
      "[1050/2600] loss=1.2182480096817017\n",
      "[1060/2600] loss=1.2257131338119507\n",
      "[1070/2600] loss=1.302520990371704\n",
      "[1080/2600] loss=1.2745366096496582\n",
      "[1090/2600] loss=1.2607015371322632\n",
      "[1100/2600] loss=1.2501505613327026\n",
      "[1110/2600] loss=1.3000774383544922\n",
      "[1120/2600] loss=1.1986689567565918\n",
      "[1130/2600] loss=1.2307384014129639\n",
      "[1140/2600] loss=1.2391878366470337\n",
      "[1150/2600] loss=1.2146453857421875\n",
      "[1160/2600] loss=1.2702512741088867\n",
      "[1170/2600] loss=1.2093955278396606\n",
      "[1180/2600] loss=1.2436515092849731\n",
      "[1190/2600] loss=1.2269598245620728\n",
      "[1200/2600] loss=1.2763549089431763\n",
      "[1200/2600] val_loss=1.4158108234405518\n",
      "[1210/2600] loss=1.2585668563842773\n",
      "[1220/2600] loss=1.2618168592453003\n",
      "[1230/2600] loss=1.2328041791915894\n",
      "[1240/2600] loss=1.1699079275131226\n",
      "[1250/2600] loss=1.2101364135742188\n",
      "[1260/2600] loss=1.1991430521011353\n",
      "[1270/2600] loss=1.1871263980865479\n",
      "[1280/2600] loss=1.2100249528884888\n",
      "[1290/2600] loss=1.2109137773513794\n",
      "[1300/2600] loss=1.1536015272140503\n",
      "[1310/2600] loss=1.1919416189193726\n",
      "[1320/2600] loss=1.1858681440353394\n",
      "[1330/2600] loss=1.1790920495986938\n",
      "[1340/2600] loss=1.1710253953933716\n",
      "[1350/2600] loss=1.1901841163635254\n",
      "[1360/2600] loss=1.2174811363220215\n",
      "[1370/2600] loss=1.1518052816390991\n",
      "[1380/2600] loss=1.1492685079574585\n",
      "[1390/2600] loss=1.2134162187576294\n",
      "[1400/2600] loss=1.1921833753585815\n",
      "[1400/2600] val_loss=1.363420009613037\n",
      "[1410/2600] loss=1.1924546957015991\n",
      "[1420/2600] loss=1.1856967210769653\n",
      "[1430/2600] loss=1.1840671300888062\n",
      "[1440/2600] loss=1.1949825286865234\n",
      "[1450/2600] loss=1.189464807510376\n",
      "[1460/2600] loss=1.2006489038467407\n",
      "[1470/2600] loss=1.1404424905776978\n",
      "[1480/2600] loss=1.1517549753189087\n",
      "[1490/2600] loss=1.1507830619812012\n",
      "[1500/2600] loss=1.1997021436691284\n",
      "[1510/2600] loss=1.141580581665039\n",
      "[1520/2600] loss=1.1540366411209106\n",
      "[1530/2600] loss=1.153841495513916\n",
      "[1540/2600] loss=1.1555519104003906\n",
      "[1550/2600] loss=1.1235389709472656\n",
      "[1560/2600] loss=1.1692559719085693\n",
      "[1570/2600] loss=1.146071195602417\n",
      "[1580/2600] loss=1.1650729179382324\n",
      "[1590/2600] loss=1.1531548500061035\n",
      "[1600/2600] loss=1.1580930948257446\n",
      "[1600/2600] val_loss=1.3337584733963013\n",
      "[1610/2600] loss=1.1859583854675293\n",
      "[1620/2600] loss=1.1334614753723145\n",
      "[1630/2600] loss=1.1799473762512207\n",
      "[1640/2600] loss=1.1096609830856323\n",
      "[1650/2600] loss=1.152717113494873\n",
      "[1660/2600] loss=1.139006495475769\n",
      "[1670/2600] loss=1.1439965963363647\n",
      "[1680/2600] loss=1.1689814329147339\n",
      "[1690/2600] loss=1.1224064826965332\n",
      "[1700/2600] loss=1.109245777130127\n",
      "[1710/2600] loss=1.16215980052948\n",
      "[1720/2600] loss=1.1249282360076904\n",
      "[1730/2600] loss=1.1143813133239746\n",
      "[1740/2600] loss=1.1252002716064453\n",
      "[1750/2600] loss=1.125363826751709\n",
      "[1760/2600] loss=1.1738020181655884\n",
      "[1770/2600] loss=1.130020022392273\n",
      "[1780/2600] loss=1.0977649688720703\n",
      "[1790/2600] loss=1.1184930801391602\n",
      "[1800/2600] loss=1.1147695779800415\n",
      "[1800/2600] val_loss=1.3243143558502197\n",
      "[1810/2600] loss=1.1238294839859009\n",
      "[1820/2600] loss=1.0875509977340698\n",
      "[1830/2600] loss=1.1136775016784668\n",
      "[1840/2600] loss=1.0730650424957275\n",
      "[1850/2600] loss=1.1214871406555176\n",
      "[1860/2600] loss=1.1158311367034912\n",
      "[1870/2600] loss=1.0931884050369263\n",
      "[1880/2600] loss=1.1306489706039429\n",
      "[1890/2600] loss=1.0991971492767334\n",
      "[1900/2600] loss=1.0951390266418457\n",
      "[1910/2600] loss=1.1186304092407227\n",
      "[1920/2600] loss=1.1140856742858887\n",
      "[1930/2600] loss=1.1213836669921875\n",
      "[1940/2600] loss=1.1305326223373413\n",
      "[1950/2600] loss=1.0947585105895996\n",
      "[1960/2600] loss=1.0934510231018066\n",
      "[1970/2600] loss=1.1226907968521118\n",
      "[1980/2600] loss=1.1327964067459106\n",
      "[1990/2600] loss=1.0840259790420532\n",
      "[2000/2600] loss=1.1082876920700073\n",
      "[2000/2600] val_loss=1.3098342418670654\n",
      "[2010/2600] loss=1.0907210111618042\n",
      "[2020/2600] loss=1.0821537971496582\n",
      "[2030/2600] loss=1.0928120613098145\n",
      "[2040/2600] loss=1.092780590057373\n",
      "[2050/2600] loss=1.0796091556549072\n",
      "[2060/2600] loss=1.080037236213684\n",
      "[2070/2600] loss=1.0685782432556152\n",
      "[2080/2600] loss=1.1112518310546875\n",
      "[2090/2600] loss=1.0910000801086426\n",
      "[2100/2600] loss=1.0769990682601929\n",
      "[2110/2600] loss=1.0983469486236572\n",
      "[2120/2600] loss=1.107535719871521\n",
      "[2130/2600] loss=1.0962270498275757\n",
      "[2140/2600] loss=1.0677045583724976\n",
      "[2150/2600] loss=1.089945673942566\n",
      "[2160/2600] loss=1.0927717685699463\n",
      "[2170/2600] loss=1.08644437789917\n",
      "[2180/2600] loss=1.1269395351409912\n",
      "[2190/2600] loss=1.0908489227294922\n",
      "[2200/2600] loss=1.0821067094802856\n",
      "[2200/2600] val_loss=1.297328233718872\n",
      "[2210/2600] loss=1.0719752311706543\n",
      "[2220/2600] loss=1.0669572353363037\n",
      "[2230/2600] loss=1.0831258296966553\n",
      "[2240/2600] loss=1.111958622932434\n",
      "[2250/2600] loss=1.0555943250656128\n",
      "[2260/2600] loss=1.0744560956954956\n",
      "[2270/2600] loss=1.0514355897903442\n",
      "[2280/2600] loss=1.0533000230789185\n",
      "[2290/2600] loss=1.066364049911499\n",
      "[2300/2600] loss=1.0617215633392334\n",
      "[2310/2600] loss=1.08223557472229\n",
      "[2320/2600] loss=1.068501591682434\n",
      "[2330/2600] loss=1.0791079998016357\n",
      "[2340/2600] loss=1.0794874429702759\n",
      "[2350/2600] loss=1.0641125440597534\n",
      "[2360/2600] loss=1.0752317905426025\n",
      "[2370/2600] loss=1.0838080644607544\n",
      "[2380/2600] loss=1.0499471426010132\n",
      "[2390/2600] loss=1.0846545696258545\n",
      "[2400/2600] loss=1.0714894533157349\n",
      "[2400/2600] val_loss=1.297881841659546\n",
      "[2410/2600] loss=1.0564475059509277\n",
      "[2420/2600] loss=1.0888230800628662\n",
      "[2430/2600] loss=1.082627773284912\n",
      "[2440/2600] loss=1.0409501791000366\n",
      "[2450/2600] loss=1.0450433492660522\n",
      "[2460/2600] loss=1.066152572631836\n",
      "[2470/2600] loss=1.0826658010482788\n",
      "[2480/2600] loss=1.0230034589767456\n",
      "[2490/2600] loss=1.0864514112472534\n",
      "[2500/2600] loss=1.0708527565002441\n",
      "[2510/2600] loss=1.0786882638931274\n",
      "[2520/2600] loss=1.0335092544555664\n",
      "[2530/2600] loss=1.0171947479248047\n",
      "[2540/2600] loss=1.038625717163086\n",
      "[2550/2600] loss=1.064149260520935\n",
      "[2560/2600] loss=1.0986440181732178\n",
      "[2570/2600] loss=1.0518251657485962\n",
      "[2580/2600] loss=1.0419930219650269\n",
      "[2590/2600] loss=1.092361330986023\n",
      "training took 91.63657712936401 seconds\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 200 # validate model every .. iterations\n",
    "log_interval = 10 # log training loss every .. iterations\n",
    "eval_iters = 20 # number of batches for loss estimation\n",
    "gradient_accumulation_steps = 5 # used to simulate larger training batch sizes\n",
    "batch_size = 96 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_size = 96 # sequence length\n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' # vocabulary\n",
    "vocab_size = len(vocab) # 40\n",
    "n_layer = 4 # number of layers\n",
    "n_head = 4 # number of attention heads\n",
    "hidden_size = 128 # layer size\n",
    "dropout = 0.1 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "learning_rate = 7e-4 # max learning rate\n",
    "max_iters = 3000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9 # for AdamW\n",
    "beta2 = 0.999 # for AdamW\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable with 0.0\n",
    "warmup_iters = 200 # how many steps to warm up for\n",
    "min_lr = 7e-5 # minimum learning rate, usually ~= learning_rate/10\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def load_data(split):\n",
    "    import re\n",
    "    \n",
    "    with open(f'trump_{split}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = text.lower() # convert to lower case\n",
    "    text = re.sub('[^a-z0-9 .!?]', ' ', text) # replace all unknown chars with ' '\n",
    "    text = re.sub(' +', ' ', text) # reduce multiple blanks to one\n",
    "    text = [vocab.index(t) for t in text]\n",
    "    text = torch.tensor(text, dtype=torch.long, device=DEVICE)\n",
    "    return text\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Random starting indices (shape: [batch_size])\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,), device=DEVICE)\n",
    "\n",
    "    # Create a 2D index tensor of shape batch_size X context_size\n",
    "    #  For each element in ix, we want to collect [i, i+1, ..., i+context_size-1].\n",
    "    #  So we broadcast-add a range of length `context_size` to each element of ix.\n",
    "    x_positions = ix.unsqueeze(-1) + torch.arange(context_size, device=DEVICE)\n",
    "    y_positions = x_positions + 1  # Shift by 1\n",
    "    x = data[x_positions]  # batch_size X context_size\n",
    "    y = data[y_positions]  # batch_size X context_size\n",
    "\n",
    "    return x, y\n",
    "    # old function\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# data, model, optimizer, etc. \n",
    "train_data = load_data('train')\n",
    "val_data = load_data('val')\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(X, Y)\n",
    "    loss.backward()\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    if iter_num % log_interval == 0:\n",
    "        print(f'[{iter_num}/{max_iters}] loss={loss.item()}')\n",
    "    if iter_num % eval_interval == 0:\n",
    "        val_loss = estimate_loss()['val']\n",
    "        print(f'[{iter_num}/{max_iters}] val_loss={val_loss}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    X, Y = get_batch('train')\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "print(f'training took {time.time()-t0} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f0e3c",
   "metadata": {},
   "source": [
    "## Exercise 8: Inference\n",
    "\n",
    "Add a method `generate` to the class `GPT` that takes arguments `x, max_new_tokens, temperature=1.0`. \n",
    "The method should take a batch of token sequences `x`, which it should extend by `max_new_tokens` new tokens generated by the model. \n",
    "Once you have computed the logits for the next token, divide them by `temperature` before applying the softmax. \n",
    "After applying the softmax, sample the next token from the resulting categorical distribution. \n",
    "Try out different values for `temperature` and compare the results to those from the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "595e921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, x, max_new_tokens, temperature=1.0):\n",
    "    self.eval()\n",
    "    N, T = x.size()\n",
    "    output = torch.zeros(size=(N, T + max_new_tokens), dtype=torch.long, device=x.device)\n",
    "    output[:, :T] = x\n",
    "    for t in range(T, T + max_new_tokens):\n",
    "        logits = self(output[:, t-context_size:t], None)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        output[:, t] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "    return output\n",
    "\n",
    "GPT.generate = generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c229c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/xfwm3xr90019fcl3by7y113c0000gn/T/ipykernel_72183/1450387794.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- temperature=0.1 ---\n",
      "                                                                              make america great and the state people and they re going to be a lot of people and they re going to be a lot of people that i don t want to be a lot of the country in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world in the world what they re going to be a lot of the country and they re going to \n",
      "--- temperature=0.2 ---\n",
      "                                                                              make america great age. i m going to get them and they re going to get the people that we re going to be a lot of the country and they re going to come in the world in this was a lot of the deal things and they re going to be so many care of the people are the statement that are the people are so many people are so many people are the best so many people are and they re going to get the people are they re going to be a lot of the country and they re going to be a lot of the people are the back the back the back a\n",
      "--- temperature=0.3 ---\n",
      "                                                                              make america great and the states and i was a lot of money and they re going to get rid of the people that i had the best a lot of our country and they re going to start the best and i said i don t know what they re going to start the wall. they re going to be so many car of a lot of the country and they re going to be a great thing to be a lot of the country i didn t be able to be a couple of the second amendment the are that s a politicians that was a lot of money that i was the second and they have to do that \n",
      "--- temperature=0.5 ---\n",
      "                                                                              make america great for and some of money and they re going to say that we need to be problem. i don t know what. and they re going to be the best that are we all on the time to the only the tape. i mean a total of the guy what s very happy in was so many for people and they re not going to have so much and we re going to be we re going to be so bad all the time to be part a lot of the next a long time to the debates in the world war and they want to controlled that i was a couple and then we have the china and ev\n",
      "--- temperature=0.6 ---\n",
      "                                                                              make america great that is the because that s so much at i don t know and to thank of it. i really a great common core. they re going to take administration and i was a really call it is so the only and we re going to start commentators in the world univision. this is a man common calls. but the tent advans serven and i don t see it right thing in might thing they want to be in the way we need to do that. we re going to come in the hear to be a business we re not going to be allowed to turn it for a total to the \n",
      "--- temperature=0.7 ---\n",
      "                                                                              make america great coud. we ll have the build or money in the elisbert that one deal. and i really deal she was said and i ll be way the other subjects bush that are big wall. they may this they re not going to start to me because they have and they said he said did it s like it all the hit was are making a great right now. and you know we have sension let s so many things that we doing this country back the press. in terms of it they had a very been taken there was and i love you know they re going to be live th\n",
      "--- temperature=0.8 ---\n",
      "                                                                              make america greate because i don t know how eneswer who strong flowers they re going to be so bad elected you know all of the governmentation. you ve got to stop. our economy because i want to come out was succept of the country and i don t know what happened even the world and and all fighting he wass there was killing that come problem. i have a good a beating disapment and you go for the nation states to have to want love them and we will this in the deduction down country for setrory or the economic entire o\n",
      "--- temperature=1.0 ---\n",
      "                                                                              make america greator s one going to happy to cut for me because i want to call it the built and walls these shas very sad to even win will. we re going to take care is that dright. i did then i m not carla. but you know them probably a everybody. of people for our natab second. we re not losing. remember conservative. they re controlled in like has you know what? i have to anlar frishefters terrorism their numbers like a crich decition memaning the world. over the third politics islam free. so any main. i come in\n"
     ]
    }
   ],
   "source": [
    "temps = [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "\n",
    "for temp in temps:\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    sentence = 'make america great'\n",
    "    x = torch.ones(size=(1, context_size), dtype=torch.long, device=DEVICE) * vocab.index(' ')\n",
    "    x[:, -len(sentence):] = torch.tensor([vocab.index(c) for c in sentence], dtype=torch.long).unsqueeze(0)\n",
    "    output = model.generate(x, 500, temperature=temp)\n",
    "    text = ''.join([vocab[i] for i in output[0].cpu().numpy()])\n",
    "\n",
    "    print(f'--- temperature={temp} ---')\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30415e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
