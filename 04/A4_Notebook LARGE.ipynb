{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e9d729",
   "metadata": {},
   "source": [
    "# Assignment 4: Training Transformers in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment we will implement and train a small transformer model and compare it to the LSTM in the previous assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089693e",
   "metadata": {},
   "source": [
    "## Exercise 1: Causal Self-Attention\n",
    "\n",
    "Write a class named `CausalSelfAttention` that derives from `nn.Module` and whose `__init__` method takes (apart from the trivial `self`) one argument `hidden_size`. Implement a method `forward` that takes an input sequence `x` of shape $(N, T, D)$ (where $N$ is batch size, $T$ is sequence length, $D$ is hidden size) and performs scaled dot-product self-attention, i.e., \n",
    "$$\n",
    "Y = \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{D}} Q K^\\top\\right) V,\n",
    "$$\n",
    "where $Q = X W_Q$ and $K = X W_K$ and $V = X W_V$ and $X \\in \\mathbb{R}^{T \\times D}$ and $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$ and softmax is applied in a row-wise manner and neglecting bias units. \n",
    "It is called self-attention because $Q, K, V$ are all computed from the same input $X$, which hence attends to itself. \n",
    "\n",
    "To have the attention be *causal* we need to make sure that we do not allow peeks into the future. That is, the output at time $t$ must be a function of the input at times $1, \\dots, t$ but no further. The score matrix $E = \\frac{1}{\\sqrt{D}} Q K^\\top$ has a shape of $T \\times T$ and the entry $e_{ij}$ measures how strong the query at time $i$ attends to the key at time $j$. Therefore, positions where $j > i$ constitute peeks into the future and we have to set the corresponding attention values (i.e., the softmax-activated score) to zero. We can do that by setting the corresponding score to `float('-inf')`, which has the advantage that the normalization is adjusted automatically by the softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28afb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device(\"mps\") if torch.mps.is_available() else \"cpu\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.WQ = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "        self.WK = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "        self.WV = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)                                                  #  (N, T, D)\n",
    "        K = self.WK(x)                                                  #  (N, T, D)\n",
    "        V = self.WV(x)                                                  #  (N, T, D)\n",
    "        Q = Q / math.sqrt(D)\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2))                     #  (N, T, D) * (N, D, T) -> (N, T, T)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)    #  (T, T)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))           #  (N, T, T)\n",
    "        attention = F.softmax(scores, dim=-1)                           #  (N, T, T)\n",
    "        out = torch.matmul(attention, V)                                #  (N, T, T) * (N, T, D) -> (N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ee970",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Write a class `MultiHeadCausalSelfAttention` that derives from `nn.Module` and extends the functionality of `CausalSelfAttention` from the previous exercise. \n",
    "The `__init__` method takes arguments `hidden_size, n_head, dropout`. `n_head` specifies the number of attention heads and `dropout` specifies the intensity for the dropout layers. \n",
    "The `forward` method should split the hidden dimension of the pre-activations (i.e., $Q, K, V$) in `n_head` equally sized parts and perform attention to these parts in parallel. \n",
    "Apply the first dropout layer direcly after the softmax. \n",
    "After the multiplication of the scores with the values, recombine the output of the distinct attention heads back into a single hidden dimension of size $D$, i.e., the resulting shape should be the shape of the input. \n",
    "Then perform an additional output projection again resulting in a hidden dimension of $D$. \n",
    "Finally, apply the second dropout layer after the output projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddee2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(MultiHeadCausalSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.WQ = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "        self.WK = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "        self.WV = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)\n",
    "        K = self.WK(x)\n",
    "        V = self.WV(x)\n",
    "        Q = Q.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        K = K.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        V = V.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        Q = Q / math.sqrt(D // self.n_head)\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(self.dropout(attention), V).permute(0, 2, 1, 3).contiguous().view(N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b329d",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Layer Perceptron\n",
    "\n",
    "Write a class `MLP` that derives from `nn.Module` and whose `__init__` method takes two arguments: `hidden_size` and `dropout`. \n",
    "It should implement a 2-layer feedforward network with `hidden_size` inputs, `4*hidden_size` hiddens, and `hidden_size` outputs. \n",
    "It should apply the GELU activation function to the hiddens and dropout to the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca16758",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W1 = nn.Linear(hidden_size, 4*hidden_size, device=DEVICE)\n",
    "        self.W2 = nn.Linear(4*hidden_size, hidden_size, device=DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.W2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645751",
   "metadata": {},
   "source": [
    "## Exercise 4: Block\n",
    "\n",
    "Write a class `Block` that derives from `nn.Module` and whose `__init__` method takes arguments `hidden_size, n_head, dropout`. \n",
    "It should apply `nn.LayerNorm`, `CausalMultiHeadSelfAttention`, `nn.LayerNorm`, `MLP` in that order and feature residual connections from the input to the output of `CausalMultiHeadSelfAttention` and from there to the output of `MLP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.attention = MultiHeadCausalSelfAttention(hidden_size, n_head, dropout)\n",
    "        self.mlp = MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(self.ln1(x)) + x\n",
    "        x = self.mlp(self.ln2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b7022",
   "metadata": {},
   "source": [
    "## Exercise 5: GPT\n",
    "\n",
    "Write a class `GPT` that derives from `nn.Module` and whose `__init__` method takes arguments `vocab_size, context_size, hidden_size, n_layer, n_head, dropout`. \n",
    "The `forward` method should take two arguments `x, y` representing sequences of input and target tokens, respectively, both of which have type `torch.long` and shape ($N$, $T$), and returns logits and loss as a tuple. \n",
    "The `GPT` module should feature two `nn.Embedding` layers, one for token embeddings and one for positional embedding, i.e., it should embed the position of the corresponding token within the input sequence. \n",
    "The positional embedding is necessary for the Transformer to determine the order of its inputs. \n",
    "Add the two embeddings and apply a dropout layer. \n",
    "Next, apply `n_layers` layers of `Block`s followed by a `nn.LayerNorm` and a `nn.Linear` (without bias) mapping to an output dimension of `vocab_size`. \n",
    "Finally, apply the cross-entropy loss function to the logits. \n",
    "To save some parameters, apply weight tying between the token embedding layer and the output layer, i.e., they should use the same weights. \n",
    "Initialize all weights using a normal distribution with a mean of zero and a standard deviation of 0.02 (except for the output layers of the `MLP`s use $0.02/\\sqrt{2 * \\mathtt{n\\_layer}}$) and all biases to zero. \n",
    "Use the argument `dropout` as intensity for all dropout layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe14a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout=0.0):\n",
    "        super(GPT, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size, device=DEVICE)\n",
    "        self.position_embedding = nn.Embedding(context_size, hidden_size, device=DEVICE)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size, bias=False, device=DEVICE)\n",
    "        self.linear.weight = self.token_embedding.weight  # weight tying\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                torch.nn.init.ones_(module.weight)\n",
    "        torch.nn.init.normal_(self.linear.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = x[:, -self.position_embedding.num_embeddings:]\n",
    "        x = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(x.shape[1], device=x.device))\n",
    "        x = x + pos_emb\n",
    "        x = self.dropout(x)\n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.linear(x)\n",
    "        logits = x\n",
    "        if y is None:\n",
    "            return logits\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc12d6",
   "metadata": {},
   "source": [
    "## Exercise 6: Optimizer\n",
    "\n",
    "Add a method `configure_optimizers` to the class `GPT` that takes arguments `weight_decay, learning_rate, betas`. \n",
    "Divide the model parameters into two groups. \n",
    "The first group consists of all parameters with at least 2 dimensions, e.g., weight/embedding matrices and uses a decay of `weight_decay`. \n",
    "The second group consists of all other parameters, e.g., biases and layer norms, and does not use weight decay.\n",
    "Construct and return a `torch.optim.AdamW` optimizer with `learning_rate` and `betas` that operates on these two parameter groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "    at_least_2_dim_params = (p for p in self.parameters() if p.ndimension() >= 2)\n",
    "    other_params = (p for p in self.parameters() if p.ndimension() < 2)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {'params': at_least_2_dim_params, 'weight_decay': weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0}\n",
    "            ], lr=learning_rate, betas=betas)\n",
    "    return optimizer\n",
    "\n",
    "GPT.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b472",
   "metadata": {},
   "source": [
    "## Exercise 7: Training\n",
    "\n",
    "In the code cell below you find some globals, helper functions, and boilerplate code. Extend the given code by a training loop that \n",
    "* stops after `max_iters` iterations\n",
    "* applies the learning rate schedule implemented in `get_lr`\n",
    "* applies gradient clipping at `grad_clip` using `torch.nn.utils.clip_grad_norm_`\n",
    "* accumulates gradients for `gradient_accumulation_steps` batches before each weight update\n",
    "* logs the training loss and learning rate every `log_interval` iterations\n",
    "* evaluates (and potentially checkpoints) the model using `estimate_loss` every `eval_iters` iterations.\n",
    "\n",
    "The provided hyperparameter values should be a good guess for training a tiny model on CPU but feel free to experiment with them as you please. In particular, if you have a GPU available, you can try to scale things up a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641a76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000] loss=3.8327651023864746\n",
      "[0/10000] val_loss=4.317932605743408\n",
      "[10/10000] loss=3.3078763484954834\n",
      "[20/10000] loss=3.0520565509796143\n",
      "[30/10000] loss=2.8776397705078125\n",
      "[40/10000] loss=2.871124267578125\n",
      "[50/10000] loss=2.828428268432617\n",
      "[60/10000] loss=2.6825506687164307\n",
      "[70/10000] loss=2.530851364135742\n",
      "[80/10000] loss=2.492325782775879\n",
      "[90/10000] loss=2.485316753387451\n",
      "[100/10000] loss=2.4421496391296387\n",
      "[110/10000] loss=2.372389793395996\n",
      "[120/10000] loss=2.3307151794433594\n",
      "[130/10000] loss=2.367492914199829\n",
      "[140/10000] loss=2.3824682235717773\n",
      "[150/10000] loss=2.3652303218841553\n",
      "[160/10000] loss=2.345362901687622\n",
      "[170/10000] loss=2.388284921646118\n",
      "[180/10000] loss=2.352450370788574\n",
      "[190/10000] loss=2.239760398864746\n",
      "[200/10000] loss=2.2890467643737793\n",
      "[200/10000] val_loss=2.3523406982421875\n",
      "[210/10000] loss=2.2736756801605225\n",
      "[220/10000] loss=2.2879414558410645\n",
      "[230/10000] loss=2.3273520469665527\n",
      "[240/10000] loss=2.2648463249206543\n",
      "[250/10000] loss=2.2978127002716064\n",
      "[260/10000] loss=2.2094783782958984\n",
      "[270/10000] loss=2.2106425762176514\n",
      "[280/10000] loss=2.138497829437256\n",
      "[290/10000] loss=2.167734384536743\n",
      "[300/10000] loss=2.1123673915863037\n",
      "[310/10000] loss=2.179342269897461\n",
      "[320/10000] loss=2.140807867050171\n",
      "[330/10000] loss=2.118295431137085\n",
      "[340/10000] loss=2.0678069591522217\n",
      "[350/10000] loss=2.0241260528564453\n",
      "[360/10000] loss=1.9332196712493896\n",
      "[370/10000] loss=1.922270655632019\n",
      "[380/10000] loss=1.9134451150894165\n",
      "[390/10000] loss=1.8338377475738525\n",
      "[400/10000] loss=1.7856627702713013\n",
      "[400/10000] val_loss=1.9831920862197876\n",
      "[410/10000] loss=1.8203052282333374\n",
      "[420/10000] loss=1.7366431951522827\n",
      "[430/10000] loss=1.689070463180542\n",
      "[440/10000] loss=1.7130184173583984\n",
      "[450/10000] loss=1.7245922088623047\n",
      "[460/10000] loss=1.6005867719650269\n",
      "[470/10000] loss=1.7060201168060303\n",
      "[480/10000] loss=1.7030067443847656\n",
      "[490/10000] loss=1.689866542816162\n",
      "[500/10000] loss=1.585371494293213\n",
      "[510/10000] loss=1.6362080574035645\n",
      "[520/10000] loss=1.552932620048523\n",
      "[530/10000] loss=1.5550216436386108\n",
      "[540/10000] loss=1.5887887477874756\n",
      "[550/10000] loss=1.6345950365066528\n",
      "[560/10000] loss=1.4951763153076172\n",
      "[570/10000] loss=1.5801235437393188\n",
      "[580/10000] loss=1.5252861976623535\n",
      "[590/10000] loss=1.5558885335922241\n",
      "[600/10000] loss=1.4724452495574951\n",
      "[600/10000] val_loss=1.634530782699585\n",
      "[610/10000] loss=1.4833171367645264\n",
      "[620/10000] loss=1.3471227884292603\n",
      "[630/10000] loss=1.4434614181518555\n",
      "[640/10000] loss=1.5583345890045166\n",
      "[650/10000] loss=1.3976112604141235\n",
      "[660/10000] loss=1.4695414304733276\n",
      "[670/10000] loss=1.3773913383483887\n",
      "[680/10000] loss=1.283652424812317\n",
      "[690/10000] loss=1.2610536813735962\n",
      "[700/10000] loss=1.4024611711502075\n",
      "[710/10000] loss=1.2744877338409424\n",
      "[720/10000] loss=1.3680310249328613\n",
      "[730/10000] loss=1.3442208766937256\n",
      "[740/10000] loss=1.311011791229248\n",
      "[750/10000] loss=1.3319271802902222\n",
      "[760/10000] loss=1.4027639627456665\n",
      "[770/10000] loss=1.3834383487701416\n",
      "[780/10000] loss=1.2509528398513794\n",
      "[790/10000] loss=1.3688372373580933\n",
      "[800/10000] loss=1.3021090030670166\n",
      "[800/10000] val_loss=1.4683903455734253\n",
      "[810/10000] loss=1.341880440711975\n",
      "[820/10000] loss=1.2585842609405518\n",
      "[830/10000] loss=1.2709323167800903\n",
      "[840/10000] loss=1.2547968626022339\n",
      "[850/10000] loss=1.286518931388855\n",
      "[860/10000] loss=1.3664004802703857\n",
      "[870/10000] loss=1.3433043956756592\n",
      "[880/10000] loss=1.3670265674591064\n",
      "[890/10000] loss=1.3921698331832886\n",
      "[900/10000] loss=1.1756292581558228\n",
      "[910/10000] loss=1.2723665237426758\n",
      "[920/10000] loss=1.2224960327148438\n",
      "[930/10000] loss=1.261330485343933\n",
      "[940/10000] loss=1.2415177822113037\n",
      "[950/10000] loss=1.307598352432251\n",
      "[960/10000] loss=1.2713555097579956\n",
      "[970/10000] loss=1.197059154510498\n",
      "[980/10000] loss=1.2937607765197754\n",
      "[990/10000] loss=1.152092695236206\n",
      "[1000/10000] loss=1.1224162578582764\n",
      "[1000/10000] val_loss=1.4187546968460083\n",
      "[1010/10000] loss=1.1862761974334717\n",
      "[1020/10000] loss=1.1812660694122314\n",
      "[1030/10000] loss=1.123439073562622\n",
      "[1040/10000] loss=1.1807923316955566\n",
      "[1050/10000] loss=1.2236645221710205\n",
      "[1060/10000] loss=1.1696276664733887\n",
      "[1070/10000] loss=1.290806531906128\n",
      "[1080/10000] loss=1.1167728900909424\n",
      "[1090/10000] loss=1.1445155143737793\n",
      "[1100/10000] loss=1.1468863487243652\n",
      "[1110/10000] loss=1.1789029836654663\n",
      "[1120/10000] loss=1.119795322418213\n",
      "[1130/10000] loss=1.2532848119735718\n",
      "[1140/10000] loss=1.2116316556930542\n",
      "[1150/10000] loss=1.1606649160385132\n",
      "[1160/10000] loss=1.1237537860870361\n",
      "[1170/10000] loss=1.1644576787948608\n",
      "[1180/10000] loss=1.1526614427566528\n",
      "[1190/10000] loss=1.1930054426193237\n",
      "[1200/10000] loss=1.126631259918213\n",
      "[1200/10000] val_loss=1.3530685901641846\n",
      "[1210/10000] loss=1.09542715549469\n",
      "[1220/10000] loss=1.0939486026763916\n",
      "[1230/10000] loss=1.1407747268676758\n",
      "[1240/10000] loss=1.1354457139968872\n",
      "[1250/10000] loss=1.053375482559204\n",
      "[1260/10000] loss=1.1210932731628418\n",
      "[1270/10000] loss=1.1448603868484497\n",
      "[1280/10000] loss=1.2036725282669067\n",
      "[1290/10000] loss=1.115767478942871\n",
      "[1300/10000] loss=1.1397236585617065\n",
      "[1310/10000] loss=1.2070918083190918\n",
      "[1320/10000] loss=1.1164021492004395\n",
      "[1330/10000] loss=1.031432867050171\n",
      "[1340/10000] loss=1.08917236328125\n",
      "[1350/10000] loss=1.0546449422836304\n",
      "[1360/10000] loss=1.1801742315292358\n",
      "[1370/10000] loss=1.0623667240142822\n",
      "[1380/10000] loss=1.0307780504226685\n",
      "[1390/10000] loss=1.10060453414917\n",
      "[1400/10000] loss=1.1276295185089111\n",
      "[1400/10000] val_loss=1.3241913318634033\n",
      "[1410/10000] loss=1.094954013824463\n",
      "[1420/10000] loss=1.1109907627105713\n",
      "[1430/10000] loss=1.168744683265686\n",
      "[1440/10000] loss=1.1136475801467896\n",
      "[1450/10000] loss=1.1226643323898315\n",
      "[1460/10000] loss=0.9442906975746155\n",
      "[1470/10000] loss=1.028639316558838\n",
      "[1480/10000] loss=1.0043072700500488\n",
      "[1490/10000] loss=1.0327175855636597\n",
      "[1500/10000] loss=1.052689790725708\n",
      "[1510/10000] loss=1.059281349182129\n",
      "[1520/10000] loss=1.057508111000061\n",
      "[1530/10000] loss=1.032020092010498\n",
      "[1540/10000] loss=1.0625511407852173\n",
      "[1550/10000] loss=1.1027528047561646\n",
      "[1560/10000] loss=1.040324091911316\n",
      "[1570/10000] loss=1.104467511177063\n",
      "[1580/10000] loss=1.0689030885696411\n",
      "[1590/10000] loss=1.0765272378921509\n",
      "[1600/10000] loss=1.0092319250106812\n",
      "[1600/10000] val_loss=1.2734379768371582\n",
      "[1610/10000] loss=1.0620156526565552\n",
      "[1620/10000] loss=1.0369532108306885\n",
      "[1630/10000] loss=0.8719029426574707\n",
      "[1640/10000] loss=1.0427368879318237\n",
      "[1650/10000] loss=1.0457992553710938\n",
      "[1660/10000] loss=0.9945467710494995\n",
      "[1670/10000] loss=0.9936906099319458\n",
      "[1680/10000] loss=0.9607799649238586\n",
      "[1690/10000] loss=1.0444868803024292\n",
      "[1700/10000] loss=1.0437904596328735\n",
      "[1710/10000] loss=0.9240627288818359\n",
      "[1720/10000] loss=1.0330647230148315\n",
      "[1730/10000] loss=1.0922940969467163\n",
      "[1740/10000] loss=0.936257541179657\n",
      "[1750/10000] loss=0.9891754984855652\n",
      "[1760/10000] loss=1.0090736150741577\n",
      "[1770/10000] loss=0.9824024438858032\n",
      "[1780/10000] loss=1.023271083831787\n",
      "[1790/10000] loss=1.1132729053497314\n",
      "[1800/10000] loss=1.0492240190505981\n",
      "[1800/10000] val_loss=1.2502667903900146\n",
      "[1810/10000] loss=0.9391739368438721\n",
      "[1820/10000] loss=1.0216104984283447\n",
      "[1830/10000] loss=1.0516480207443237\n",
      "[1840/10000] loss=0.9948891401290894\n",
      "[1850/10000] loss=0.9076608419418335\n",
      "[1860/10000] loss=1.0221730470657349\n",
      "[1870/10000] loss=1.0562971830368042\n",
      "[1880/10000] loss=1.003639817237854\n",
      "[1890/10000] loss=1.04494309425354\n",
      "[1900/10000] loss=0.9696453213691711\n",
      "[1910/10000] loss=0.9513216614723206\n",
      "[1920/10000] loss=1.0008788108825684\n",
      "[1930/10000] loss=0.9826763868331909\n",
      "[1940/10000] loss=0.9846245050430298\n",
      "[1950/10000] loss=0.9462327361106873\n",
      "[1960/10000] loss=1.041910171508789\n",
      "[1970/10000] loss=0.9003393650054932\n",
      "[1980/10000] loss=0.8922881484031677\n",
      "[1990/10000] loss=0.9924097061157227\n",
      "[2000/10000] loss=0.9823469519615173\n",
      "[2000/10000] val_loss=1.2329319715499878\n",
      "[2010/10000] loss=1.0453442335128784\n",
      "[2020/10000] loss=0.9185269474983215\n",
      "[2030/10000] loss=0.8928934335708618\n",
      "[2040/10000] loss=0.8479831218719482\n",
      "[2050/10000] loss=0.8850199580192566\n",
      "[2060/10000] loss=0.8994244337081909\n",
      "[2070/10000] loss=0.9572940468788147\n",
      "[2080/10000] loss=0.9656023979187012\n",
      "[2090/10000] loss=1.0115164518356323\n",
      "[2100/10000] loss=0.9207028150558472\n",
      "[2110/10000] loss=0.9152237176895142\n",
      "[2120/10000] loss=0.8755110502243042\n",
      "[2130/10000] loss=0.9327182173728943\n",
      "[2140/10000] loss=0.8981114625930786\n",
      "[2150/10000] loss=0.9114853143692017\n",
      "[2160/10000] loss=0.8572365641593933\n",
      "[2170/10000] loss=0.9325439929962158\n",
      "[2180/10000] loss=0.9655905961990356\n",
      "[2190/10000] loss=0.7794684171676636\n",
      "[2200/10000] loss=0.8905458450317383\n",
      "[2200/10000] val_loss=1.2409837245941162\n",
      "[2210/10000] loss=0.9336296319961548\n",
      "[2220/10000] loss=0.938167929649353\n",
      "[2230/10000] loss=0.8766485452651978\n",
      "[2240/10000] loss=0.9097707867622375\n",
      "[2250/10000] loss=0.9866771697998047\n",
      "[2260/10000] loss=0.9544339776039124\n",
      "[2270/10000] loss=0.9207520484924316\n",
      "[2280/10000] loss=0.8698888421058655\n",
      "[2290/10000] loss=0.8949252367019653\n",
      "[2300/10000] loss=0.9928652048110962\n",
      "[2310/10000] loss=0.825907289981842\n",
      "[2320/10000] loss=0.8818501830101013\n",
      "[2330/10000] loss=0.9050630927085876\n",
      "[2340/10000] loss=0.9613543152809143\n",
      "[2350/10000] loss=0.8981726765632629\n",
      "[2360/10000] loss=0.8577426075935364\n",
      "[2370/10000] loss=0.876580536365509\n",
      "[2380/10000] loss=0.9026256203651428\n",
      "[2390/10000] loss=0.878677248954773\n",
      "[2400/10000] loss=0.9472694993019104\n",
      "[2400/10000] val_loss=1.2043302059173584\n",
      "[2410/10000] loss=0.8849573731422424\n",
      "[2420/10000] loss=0.8698586225509644\n",
      "[2430/10000] loss=0.8941534161567688\n",
      "[2440/10000] loss=0.8552768230438232\n",
      "[2450/10000] loss=0.8228130340576172\n",
      "[2460/10000] loss=0.850102424621582\n",
      "[2470/10000] loss=0.8441717028617859\n",
      "[2480/10000] loss=0.8745374083518982\n",
      "[2490/10000] loss=0.8147202730178833\n",
      "[2500/10000] loss=0.7760091423988342\n",
      "[2510/10000] loss=0.805406928062439\n",
      "[2520/10000] loss=0.895129919052124\n",
      "[2530/10000] loss=0.8848169445991516\n",
      "[2540/10000] loss=0.8656558394432068\n",
      "[2550/10000] loss=0.8773079514503479\n",
      "[2560/10000] loss=0.9117988348007202\n",
      "[2570/10000] loss=0.8300171494483948\n",
      "[2580/10000] loss=0.8380043506622314\n",
      "[2590/10000] loss=0.8419846296310425\n",
      "[2600/10000] loss=0.8092742562294006\n",
      "[2600/10000] val_loss=1.166407823562622\n",
      "[2610/10000] loss=0.8352418541908264\n",
      "[2620/10000] loss=0.8860066533088684\n",
      "[2630/10000] loss=0.7891386151313782\n",
      "[2640/10000] loss=0.7978878021240234\n",
      "[2650/10000] loss=0.8471264243125916\n",
      "[2660/10000] loss=0.8547272682189941\n",
      "[2670/10000] loss=0.8329023718833923\n",
      "[2680/10000] loss=0.8779584169387817\n",
      "[2690/10000] loss=0.8304296135902405\n",
      "[2700/10000] loss=0.8980220556259155\n",
      "[2710/10000] loss=0.8161097764968872\n",
      "[2720/10000] loss=0.7613641619682312\n",
      "[2730/10000] loss=0.854819655418396\n",
      "[2740/10000] loss=0.8416852355003357\n",
      "[2750/10000] loss=0.7389364838600159\n",
      "[2760/10000] loss=0.7265158295631409\n",
      "[2770/10000] loss=0.7858767509460449\n",
      "[2780/10000] loss=0.89621502161026\n",
      "[2790/10000] loss=0.7527675628662109\n",
      "[2800/10000] loss=0.8150895833969116\n",
      "[2800/10000] val_loss=1.1855846643447876\n",
      "[2810/10000] loss=0.7268385291099548\n",
      "[2820/10000] loss=0.787307858467102\n",
      "[2830/10000] loss=0.8841571807861328\n",
      "[2840/10000] loss=0.7758002877235413\n",
      "[2850/10000] loss=0.8066811561584473\n",
      "[2860/10000] loss=0.7719966173171997\n",
      "[2870/10000] loss=0.81678706407547\n",
      "[2880/10000] loss=0.7390488982200623\n",
      "[2890/10000] loss=0.8891369104385376\n",
      "[2900/10000] loss=0.8170652985572815\n",
      "[2910/10000] loss=0.7980882525444031\n",
      "[2920/10000] loss=0.7614551186561584\n",
      "[2930/10000] loss=0.7231658101081848\n",
      "[2940/10000] loss=0.7517153024673462\n",
      "[2950/10000] loss=0.7678018808364868\n",
      "[2960/10000] loss=0.7892541885375977\n",
      "[2970/10000] loss=0.7748374342918396\n",
      "[2980/10000] loss=0.6598567962646484\n",
      "[2990/10000] loss=0.7293712496757507\n",
      "[3000/10000] loss=0.7643717527389526\n",
      "[3000/10000] val_loss=1.1671562194824219\n",
      "[3010/10000] loss=0.7264901399612427\n",
      "[3020/10000] loss=0.7726809978485107\n",
      "[3030/10000] loss=0.7376478314399719\n",
      "[3040/10000] loss=0.7170776128768921\n",
      "[3050/10000] loss=0.7672026753425598\n",
      "[3060/10000] loss=0.7349833846092224\n",
      "[3070/10000] loss=0.6706617474555969\n",
      "[3080/10000] loss=0.8406157493591309\n",
      "[3090/10000] loss=0.7554134130477905\n",
      "[3100/10000] loss=0.753061830997467\n",
      "[3110/10000] loss=0.7517362236976624\n",
      "[3120/10000] loss=0.6793184876441956\n",
      "[3130/10000] loss=0.710010290145874\n",
      "[3140/10000] loss=0.8473314642906189\n",
      "[3150/10000] loss=0.7701749205589294\n",
      "[3160/10000] loss=0.72837895154953\n",
      "[3170/10000] loss=0.7151787281036377\n",
      "[3180/10000] loss=0.7636340260505676\n",
      "[3190/10000] loss=0.8066902756690979\n",
      "[3200/10000] loss=0.7136454582214355\n",
      "[3200/10000] val_loss=1.1714718341827393\n",
      "[3210/10000] loss=0.7158663272857666\n",
      "[3220/10000] loss=0.6760650277137756\n",
      "[3230/10000] loss=0.7343877553939819\n",
      "[3240/10000] loss=0.7051430940628052\n",
      "[3250/10000] loss=0.6583646535873413\n",
      "[3260/10000] loss=0.71676105260849\n",
      "[3270/10000] loss=0.7273625731468201\n",
      "[3280/10000] loss=0.6767435669898987\n",
      "[3290/10000] loss=0.7395423054695129\n",
      "[3300/10000] loss=0.6266259551048279\n",
      "[3310/10000] loss=0.7540799379348755\n",
      "[3320/10000] loss=0.6454107165336609\n",
      "[3330/10000] loss=0.6596558690071106\n",
      "[3340/10000] loss=0.7525420188903809\n",
      "[3350/10000] loss=0.6239409446716309\n",
      "[3360/10000] loss=0.710078239440918\n",
      "[3370/10000] loss=0.7477497458457947\n",
      "[3380/10000] loss=0.7988988757133484\n",
      "[3390/10000] loss=0.6455960273742676\n",
      "[3400/10000] loss=0.7089256048202515\n",
      "[3400/10000] val_loss=1.1799752712249756\n",
      "[3410/10000] loss=0.583125650882721\n",
      "[3420/10000] loss=0.6047605276107788\n",
      "[3430/10000] loss=0.7212890982627869\n",
      "[3440/10000] loss=0.6998028755187988\n",
      "[3450/10000] loss=0.6829476356506348\n",
      "[3460/10000] loss=0.5804891586303711\n",
      "[3470/10000] loss=0.7787431478500366\n",
      "[3480/10000] loss=0.6591565012931824\n",
      "[3490/10000] loss=0.6645582318305969\n",
      "[3500/10000] loss=0.6382454633712769\n",
      "[3510/10000] loss=0.7253060340881348\n",
      "[3520/10000] loss=0.6097137928009033\n",
      "[3530/10000] loss=0.6166955232620239\n",
      "[3540/10000] loss=0.661854088306427\n",
      "[3550/10000] loss=0.5966936349868774\n",
      "[3560/10000] loss=0.6125661134719849\n",
      "[3570/10000] loss=0.6267098784446716\n",
      "[3580/10000] loss=0.5706294775009155\n",
      "[3590/10000] loss=0.6581032872200012\n",
      "[3600/10000] loss=0.6758062839508057\n",
      "[3600/10000] val_loss=1.2308664321899414\n",
      "[3610/10000] loss=0.6897619366645813\n",
      "[3620/10000] loss=0.6251161694526672\n",
      "[3630/10000] loss=0.6410309076309204\n",
      "[3640/10000] loss=0.6188722848892212\n",
      "[3650/10000] loss=0.6192440390586853\n",
      "[3660/10000] loss=0.6664507389068604\n",
      "[3670/10000] loss=0.5664721727371216\n",
      "[3680/10000] loss=0.662101686000824\n",
      "[3690/10000] loss=0.5734156966209412\n",
      "[3700/10000] loss=0.640400230884552\n",
      "[3710/10000] loss=0.5405719876289368\n",
      "[3720/10000] loss=0.5328928828239441\n",
      "[3730/10000] loss=0.6820666790008545\n",
      "[3740/10000] loss=0.6314569115638733\n",
      "[3750/10000] loss=0.5326748490333557\n",
      "[3760/10000] loss=0.6257601380348206\n",
      "[3770/10000] loss=0.6491333246231079\n",
      "[3780/10000] loss=0.569937527179718\n",
      "[3790/10000] loss=0.6605080366134644\n",
      "[3800/10000] loss=0.5986691117286682\n",
      "[3800/10000] val_loss=1.174031138420105\n",
      "[3810/10000] loss=0.6324312090873718\n",
      "[3820/10000] loss=0.5788000226020813\n",
      "[3830/10000] loss=0.5439646244049072\n",
      "[3840/10000] loss=0.5017789006233215\n",
      "[3850/10000] loss=0.5519493222236633\n",
      "[3860/10000] loss=0.6445334553718567\n",
      "[3870/10000] loss=0.5315204858779907\n",
      "[3880/10000] loss=0.6053377985954285\n",
      "[3890/10000] loss=0.6276270747184753\n",
      "[3900/10000] loss=0.4847158193588257\n",
      "[3910/10000] loss=0.5983043313026428\n",
      "[3920/10000] loss=0.5685454607009888\n",
      "[3930/10000] loss=0.6268225908279419\n",
      "[3940/10000] loss=0.54054194688797\n",
      "[3950/10000] loss=0.5533750653266907\n",
      "[3960/10000] loss=0.5174474716186523\n",
      "[3970/10000] loss=0.5658126473426819\n",
      "[3980/10000] loss=0.5544121861457825\n",
      "[3990/10000] loss=0.5078631043434143\n",
      "[4000/10000] loss=0.5502028465270996\n",
      "[4000/10000] val_loss=1.246099591255188\n",
      "[4010/10000] loss=0.5469478368759155\n",
      "[4020/10000] loss=0.4892182946205139\n",
      "[4030/10000] loss=0.5824418067932129\n",
      "[4040/10000] loss=0.5489975810050964\n",
      "[4050/10000] loss=0.5099100470542908\n",
      "[4060/10000] loss=0.45445024967193604\n",
      "[4070/10000] loss=0.541982889175415\n",
      "[4080/10000] loss=0.4588505029678345\n",
      "[4090/10000] loss=0.45863980054855347\n",
      "[4100/10000] loss=0.5234876275062561\n",
      "[4110/10000] loss=0.4663744270801544\n",
      "[4120/10000] loss=0.47041311860084534\n",
      "[4130/10000] loss=0.5624364018440247\n",
      "[4140/10000] loss=0.6053023338317871\n",
      "[4150/10000] loss=0.5230448246002197\n",
      "[4160/10000] loss=0.5964387655258179\n",
      "[4170/10000] loss=0.571519136428833\n",
      "[4180/10000] loss=0.5244208574295044\n",
      "[4190/10000] loss=0.45620256662368774\n",
      "[4200/10000] loss=0.474911630153656\n",
      "[4200/10000] val_loss=1.1819896697998047\n",
      "[4210/10000] loss=0.48130175471305847\n",
      "[4220/10000] loss=0.5303674340248108\n",
      "[4230/10000] loss=0.4698510766029358\n",
      "[4240/10000] loss=0.4128188192844391\n",
      "[4250/10000] loss=0.6129844188690186\n",
      "[4260/10000] loss=0.4612289369106293\n",
      "[4270/10000] loss=0.5475163459777832\n",
      "[4280/10000] loss=0.49121248722076416\n",
      "[4290/10000] loss=0.45221439003944397\n",
      "[4300/10000] loss=0.4897001087665558\n",
      "[4310/10000] loss=0.5059571266174316\n",
      "[4320/10000] loss=0.4884977638721466\n",
      "[4330/10000] loss=0.42591771483421326\n",
      "[4340/10000] loss=0.4482725262641907\n",
      "[4350/10000] loss=0.4953325390815735\n",
      "[4360/10000] loss=0.43000006675720215\n",
      "[4370/10000] loss=0.49407511949539185\n",
      "[4380/10000] loss=0.5493383407592773\n",
      "[4390/10000] loss=0.5166306495666504\n",
      "[4400/10000] loss=0.49978354573249817\n",
      "[4400/10000] val_loss=1.158571481704712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 99\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x)) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mMultiHeadCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m Q \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(D \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)\n\u001b[1;32m     20\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Q, K\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 21\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     23\u001b[0m attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_interval = 200 # validate model every .. iterations\n",
    "log_interval = 10 # log training loss every .. iterations\n",
    "eval_iters = 20 # number of batches for loss estimation\n",
    "gradient_accumulation_steps = 2 # used to simulate larger training batch sizes\n",
    "batch_size = 16 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_size = 128 # sequence length\n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' # vocabulary\n",
    "vocab_size = len(vocab) # 40\n",
    "n_layer = 12 # number of layers\n",
    "n_head = 12 # number of attention heads\n",
    "hidden_size = 768 # layer size\n",
    "dropout = 0.1 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "learning_rate = 3e-4 # max learning rate\n",
    "max_iters = 10_000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9 # for AdamW\n",
    "beta2 = 0.999 # for AdamW\n",
    "grad_clip = 0.8 # clip gradients at this value, or disable with 0.0\n",
    "warmup_iters = 500 # how many steps to warm up for\n",
    "min_lr = 3e-5 # minimum learning rate, usually ~= learning_rate/10\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def load_data(split):\n",
    "    import re\n",
    "    \n",
    "    with open(f'trump_{split}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = text.lower() # convert to lower case\n",
    "    text = re.sub('[^a-z0-9 .!?]', ' ', text) # replace all unknown chars with ' '\n",
    "    text = re.sub(' +', ' ', text) # reduce multiple blanks to one\n",
    "    text = [vocab.index(t) for t in text]\n",
    "    text = torch.tensor(text, dtype=torch.long, device=DEVICE)\n",
    "    return text\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Random starting indices (shape: [batch_size])\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,), device=DEVICE)\n",
    "\n",
    "    # Create a 2D index tensor of shape batch_size X context_size\n",
    "    #  For each element in ix, we want to collect [i, i+1, ..., i+context_size-1].\n",
    "    #  So we broadcast-add a range of length `context_size` to each element of ix.\n",
    "    x_positions = ix.unsqueeze(-1) + torch.arange(context_size, device=DEVICE)\n",
    "    y_positions = x_positions + 1  # Shift by 1\n",
    "    x = data[x_positions]  # batch_size X context_size\n",
    "    y = data[y_positions]  # batch_size X context_size\n",
    "\n",
    "    return x, y\n",
    "    # old function\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# data, model, optimizer, etc. \n",
    "train_data = load_data('train')\n",
    "val_data = load_data('val')\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(X, Y)\n",
    "    loss.backward()\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    if iter_num % log_interval == 0:\n",
    "        print(f'[{iter_num}/{max_iters}] loss={loss.item()}')\n",
    "    if iter_num % eval_interval == 0:\n",
    "        val_loss = estimate_loss()['val']\n",
    "        print(f'[{iter_num}/{max_iters}] val_loss={val_loss}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    X, Y = get_batch('train')\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "print(f'training took {time.time()-t0} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562df06",
   "metadata": {},
   "source": [
    "I early stopped training here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f0e3c",
   "metadata": {},
   "source": [
    "## Exercise 8: Inference\n",
    "\n",
    "Add a method `generate` to the class `GPT` that takes arguments `x, max_new_tokens, temperature=1.0`. \n",
    "The method should take a batch of token sequences `x`, which it should extend by `max_new_tokens` new tokens generated by the model. \n",
    "Once you have computed the logits for the next token, divide them by `temperature` before applying the softmax. \n",
    "After applying the softmax, sample the next token from the resulting categorical distribution. \n",
    "Try out different values for `temperature` and compare the results to those from the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595e921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, x, max_new_tokens, temperature=1.0):\n",
    "    self.eval()\n",
    "    N, T = x.size()\n",
    "    output = torch.zeros(size=(N, T + max_new_tokens), dtype=torch.long, device=x.device)\n",
    "    output[:, :T] = x\n",
    "    for t in range(T, T + max_new_tokens):\n",
    "        logits = self(output[:, t-context_size:t], None)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        output[:, t] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "    return output\n",
    "\n",
    "GPT.generate = generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c229c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- temperature=0.1 ---\n",
      "                                                                                                              make america great again. the days of deadly ignorance will end that we will not be able to deal and all of that. but we have to rebuild our country. our country is going to be one of the countries of the way establishment people in and we ve seen in the polls. no i don t think i ve even say that before. i ve already gone after this before. the election is not bad. i said no no no. i don t think i think it s a man of the other reason i think it s going to be very hard. i mean i think it s a much bigger than anybo\n",
      "--- temperature=0.2 ---\n",
      "                                                                                                              make america great again. the arm you know the dreamers workers again. the margins against the world. the old post office we have to rebuild our country. we have a presidential race to do a fabulous job. we have a lot of foolish people. we ve got to find our prisoners back. we ve should have had the prisoners back because we have a lot of friends that are against guns and i thought we had the greatest business do no matter of us from the whole community. i m not a conservative conservative but no conservative and\n",
      "--- temperature=0.3 ---\n",
      "                                                                                                              make america great again. we are going to make america great again. we re going to make america great again. and i love you all thank you very much. great evening. i ll give you on television and i think i love the jobs of building it. i love the jobs of building it. i love the jobs of building it. i love the jobs of building it. but i may may buildings because i have to say no i have to tell you i have a room life nashville. i have a rigged system for me right now back for many years. i ll be brought about this \n",
      "--- temperature=0.5 ---\n",
      "                                                                                                              make america great again. thank you. thank you. thank you very much everybody! thank you. thank you very much everybody. this was an amazing evening. this was an eletement of this we had to do something about it s an economy poll. but it s the choice. it s a movement. and you know what? it s ridiculous. it s totally under budget in on television by the way. i m the only one one of the great people i know. i know the great abuser. i m a budget nobody s donald down but it s more money than they say but i m the most\n",
      "--- temperature=0.6 ---\n",
      "                                                                                                              make america great again. we are going to make america so far than it s a big area. so we are going to make a place we are going to take care of the second amendment. we re going to put america back to work. we re going to make america great again. we re going to make america great again. we re going to make america great again. we re going to make america great again. we re going to make america great again. we re going to make america great again. we re going to make america great again. so important so ..... w\n",
      "--- temperature=0.7 ---\n",
      "                                                                                                              make america great again. we are going to make america great again. the days of deadly ignorance will endorse so many others. we ve done so well with the evangelicals because you can uparl is so easy. you have to do it right from here and from here and from here and from here lies and i see all talk about the border. they ve because the border patrol knows where they re talking about the miners and they re talking about how to do their jobs. they re not taking any back off. they re buying for them they re not tak\n",
      "--- temperature=0.8 ---\n",
      "                                                                                                              make america great again. we re going to make a deal. and finally i had a great businessapolve bleg we re going to get rid of deductions which i really am. you know when i look at at the evangelicals and you ve seen almost a young child and i ve seen some of the jobs because it says about it. you know we re talking about trying to get rid of it. i promise i d like to change and i put up the money. i made up financial and did the race sancing and i m going to bring trade a lot of reasons and i built a great compan\n",
      "--- temperature=1.0 ---\n",
      "                                                                                                              make america great again. we are going to say we re going to make great deals and fair deals and that s great. we re going to make great deals and fair. and a lot of beautiful company great company where they gave jace right away they love order. the countries that they re killing us our money and they rain back you because the other day two years afradministration with our government secrets with me book are wonderful child. they re not a brilliant job. they re raised the money for the vets and nabya. not  but s\n"
     ]
    }
   ],
   "source": [
    "temps = [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "\n",
    "for temp in temps:\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    sentence = 'make america great'\n",
    "    x = torch.ones(size=(1, context_size), dtype=torch.long, device=DEVICE) * vocab.index(' ')\n",
    "    x[:, -len(sentence):] = torch.tensor([vocab.index(c) for c in sentence], dtype=torch.long).unsqueeze(0)\n",
    "    output = model.generate(x, 500, temperature=temp)\n",
    "    text = ''.join([vocab[i] for i in output[0].cpu().numpy()])\n",
    "\n",
    "    print(f'--- temperature={temp} ---')\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
