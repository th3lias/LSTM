{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e9d729",
   "metadata": {},
   "source": [
    "# Assignment 4: Training Transformers in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment we will implement and train a small transformer model and compare it to the LSTM in the previous assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089693e",
   "metadata": {},
   "source": [
    "## Exercise 1: Causal Self-Attention\n",
    "\n",
    "Write a class named `CausalSelfAttention` that derives from `nn.Module` and whose `__init__` method takes (apart from the trivial `self`) one argument `hidden_size`. Implement a method `forward` that takes an input sequence `x` of shape $(N, T, D)$ (where $N$ is batch size, $T$ is sequence length, $D$ is hidden size) and performs scaled dot-product self-attention, i.e., \n",
    "$$\n",
    "Y = \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{D}} Q K^\\top\\right) V,\n",
    "$$\n",
    "where $Q = X W_Q$ and $K = X W_K$ and $V = X W_V$ and $X \\in \\mathbb{R}^{T \\times D}$ and $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$ and softmax is applied in a row-wise manner and neglecting bias units. \n",
    "It is called self-attention because $Q, K, V$ are all computed from the same input $X$, which hence attends to itself. \n",
    "\n",
    "To have the attention be *causal* we need to make sure that we do not allow peeks into the future. That is, the output at time $t$ must be a function of the input at times $1, \\dots, t$ but no further. The score matrix $E = \\frac{1}{\\sqrt{D}} Q K^\\top$ has a shape of $T \\times T$ and the entry $e_{ij}$ measures how strong the query at time $i$ attends to the key at time $j$. Therefore, positions where $j > i$ constitute peeks into the future and we have to set the corresponding attention values (i.e., the softmax-activated score) to zero. We can do that by setting the corresponding score to `float('-inf')`, which has the advantage that the normalization is adjusted automatically by the softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28afb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device(\"mps\") if torch.mps.is_available() else \"cpu\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.WQ = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "        self.WK = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "        self.WV = nn.Linear(hidden_dim, hidden_dim, bias=False, device=DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)                                                  #  (N, T, D)\n",
    "        K = self.WK(x)                                                  #  (N, T, D)\n",
    "        V = self.WV(x)                                                  #  (N, T, D)\n",
    "        Q = Q / math.sqrt(D)\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2))                     #  (N, T, D) * (N, D, T) -> (N, T, T)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)    #  (T, T)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))           #  (N, T, T)\n",
    "        attention = F.softmax(scores, dim=-1)                           #  (N, T, T)\n",
    "        out = torch.matmul(attention, V)                                #  (N, T, T) * (N, T, D) -> (N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ee970",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Write a class `MultiHeadCausalSelfAttention` that derives from `nn.Module` and extends the functionality of `CausalSelfAttention` from the previous exercise. \n",
    "The `__init__` method takes arguments `hidden_size, n_head, dropout`. `n_head` specifies the number of attention heads and `dropout` specifies the intensity for the dropout layers. \n",
    "The `forward` method should split the hidden dimension of the pre-activations (i.e., $Q, K, V$) in `n_head` equally sized parts and perform attention to these parts in parallel. \n",
    "Apply the first dropout layer direcly after the softmax. \n",
    "After the multiplication of the scores with the values, recombine the output of the distinct attention heads back into a single hidden dimension of size $D$, i.e., the resulting shape should be the shape of the input. \n",
    "Then perform an additional output projection again resulting in a hidden dimension of $D$. \n",
    "Finally, apply the second dropout layer after the output projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddee2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(MultiHeadCausalSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.WQ = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "        self.WK = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "        self.WV = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)\n",
    "        K = self.WK(x)\n",
    "        V = self.WV(x)\n",
    "        Q = Q.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        K = K.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        V = V.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        Q = Q / math.sqrt(D // self.n_head)\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(self.dropout(attention), V).permute(0, 2, 1, 3).contiguous().view(N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b329d",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Layer Perceptron\n",
    "\n",
    "Write a class `MLP` that derives from `nn.Module` and whose `__init__` method takes two arguments: `hidden_size` and `dropout`. \n",
    "It should implement a 2-layer feedforward network with `hidden_size` inputs, `4*hidden_size` hiddens, and `hidden_size` outputs. \n",
    "It should apply the GELU activation function to the hiddens and dropout to the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca16758",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W1 = nn.Linear(hidden_size, 4*hidden_size, device=DEVICE)\n",
    "        self.W2 = nn.Linear(4*hidden_size, hidden_size, device=DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.W2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645751",
   "metadata": {},
   "source": [
    "## Exercise 4: Block\n",
    "\n",
    "Write a class `Block` that derives from `nn.Module` and whose `__init__` method takes arguments `hidden_size, n_head, dropout`. \n",
    "It should apply `nn.LayerNorm`, `CausalMultiHeadSelfAttention`, `nn.LayerNorm`, `MLP` in that order and feature residual connections from the input to the output of `CausalMultiHeadSelfAttention` and from there to the output of `MLP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.attention = MultiHeadCausalSelfAttention(hidden_size, n_head, dropout)\n",
    "        self.mlp = MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(self.ln1(x)) + x\n",
    "        x = self.mlp(self.ln2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b7022",
   "metadata": {},
   "source": [
    "## Exercise 5: GPT\n",
    "\n",
    "Write a class `GPT` that derives from `nn.Module` and whose `__init__` method takes arguments `vocab_size, context_size, hidden_size, n_layer, n_head, dropout`. \n",
    "The `forward` method should take two arguments `x, y` representing sequences of input and target tokens, respectively, both of which have type `torch.long` and shape ($N$, $T$), and returns logits and loss as a tuple. \n",
    "The `GPT` module should feature two `nn.Embedding` layers, one for token embeddings and one for positional embedding, i.e., it should embed the position of the corresponding token within the input sequence. \n",
    "The positional embedding is necessary for the Transformer to determine the order of its inputs. \n",
    "Add the two embeddings and apply a dropout layer. \n",
    "Next, apply `n_layers` layers of `Block`s followed by a `nn.LayerNorm` and a `nn.Linear` (without bias) mapping to an output dimension of `vocab_size`. \n",
    "Finally, apply the cross-entropy loss function to the logits. \n",
    "To save some parameters, apply weight tying between the token embedding layer and the output layer, i.e., they should use the same weights. \n",
    "Initialize all weights using a normal distribution with a mean of zero and a standard deviation of 0.02 (except for the output layers of the `MLP`s use $0.02/\\sqrt{2 * \\mathtt{n\\_layer}}$) and all biases to zero. \n",
    "Use the argument `dropout` as intensity for all dropout layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe14a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout=0.0):\n",
    "        super(GPT, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size, device=DEVICE)\n",
    "        self.position_embedding = nn.Embedding(context_size, hidden_size, device=DEVICE)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(hidden_size, device=DEVICE)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size, bias=False, device=DEVICE)\n",
    "        self.linear.weight = self.token_embedding.weight  # weight tying\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                torch.nn.init.ones_(module.weight)\n",
    "        torch.nn.init.normal_(self.linear.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = x[:, -self.position_embedding.num_embeddings:]\n",
    "        x = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(x.shape[1], device=x.device))\n",
    "        x = x + pos_emb\n",
    "        x = self.dropout(x)\n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.linear(x)\n",
    "        logits = x\n",
    "        if y is None:\n",
    "            return logits\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc12d6",
   "metadata": {},
   "source": [
    "## Exercise 6: Optimizer\n",
    "\n",
    "Add a method `configure_optimizers` to the class `GPT` that takes arguments `weight_decay, learning_rate, betas`. \n",
    "Divide the model parameters into two groups. \n",
    "The first group consists of all parameters with at least 2 dimensions, e.g., weight/embedding matrices and uses a decay of `weight_decay`. \n",
    "The second group consists of all other parameters, e.g., biases and layer norms, and does not use weight decay.\n",
    "Construct and return a `torch.optim.AdamW` optimizer with `learning_rate` and `betas` that operates on these two parameter groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "    at_least_2_dim_params = (p for p in self.parameters() if p.ndimension() >= 2)\n",
    "    other_params = (p for p in self.parameters() if p.ndimension() < 2)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {'params': at_least_2_dim_params, 'weight_decay': weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0}\n",
    "            ], lr=learning_rate, betas=betas)\n",
    "    return optimizer\n",
    "\n",
    "GPT.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b472",
   "metadata": {},
   "source": [
    "## Exercise 7: Training\n",
    "\n",
    "In the code cell below you find some globals, helper functions, and boilerplate code. Extend the given code by a training loop that \n",
    "* stops after `max_iters` iterations\n",
    "* applies the learning rate schedule implemented in `get_lr`\n",
    "* applies gradient clipping at `grad_clip` using `torch.nn.utils.clip_grad_norm_`\n",
    "* accumulates gradients for `gradient_accumulation_steps` batches before each weight update\n",
    "* logs the training loss and learning rate every `log_interval` iterations\n",
    "* evaluates (and potentially checkpoints) the model using `estimate_loss` every `eval_iters` iterations.\n",
    "\n",
    "The provided hyperparameter values should be a good guess for training a tiny model on CPU but feel free to experiment with them as you please. In particular, if you have a GPU available, you can try to scale things up a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641a76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/3000] loss=3.697464942932129\n",
      "[0/3000] val_loss=3.2638392448425293\n",
      "[10/3000] loss=3.110136032104492\n",
      "[20/3000] loss=2.986957550048828\n",
      "[30/3000] loss=2.9461991786956787\n",
      "[40/3000] loss=2.9075405597686768\n",
      "[50/3000] loss=2.7792675495147705\n",
      "[60/3000] loss=2.639468193054199\n",
      "[70/3000] loss=2.495661973953247\n",
      "[80/3000] loss=2.4281389713287354\n",
      "[90/3000] loss=2.3740949630737305\n",
      "[100/3000] loss=2.339038610458374\n",
      "[110/3000] loss=2.3214104175567627\n",
      "[120/3000] loss=2.3209080696105957\n",
      "[130/3000] loss=2.306276559829712\n",
      "[140/3000] loss=2.300175905227661\n",
      "[150/3000] loss=2.251856803894043\n",
      "[160/3000] loss=2.26835036277771\n",
      "[170/3000] loss=2.2271015644073486\n",
      "[180/3000] loss=2.2148783206939697\n",
      "[190/3000] loss=2.1958532333374023\n",
      "[200/3000] loss=2.189671277999878\n",
      "[200/3000] val_loss=2.2390482425689697\n",
      "[210/3000] loss=2.133718490600586\n",
      "[220/3000] loss=2.055171251296997\n",
      "[230/3000] loss=2.0254712104797363\n",
      "[240/3000] loss=1.954790711402893\n",
      "[250/3000] loss=1.8800907135009766\n",
      "[260/3000] loss=1.8483844995498657\n",
      "[270/3000] loss=1.7782230377197266\n",
      "[280/3000] loss=1.7624331712722778\n",
      "[290/3000] loss=1.6744190454483032\n",
      "[300/3000] loss=1.6228049993515015\n",
      "[310/3000] loss=1.6487079858779907\n",
      "[320/3000] loss=1.611804485321045\n",
      "[330/3000] loss=1.5607479810714722\n",
      "[340/3000] loss=1.5217046737670898\n",
      "[350/3000] loss=1.523194670677185\n",
      "[360/3000] loss=1.495773196220398\n",
      "[370/3000] loss=1.473371148109436\n",
      "[380/3000] loss=1.4533456563949585\n",
      "[390/3000] loss=1.3883180618286133\n",
      "[400/3000] loss=1.3816438913345337\n",
      "[400/3000] val_loss=1.5455055236816406\n",
      "[410/3000] loss=1.3603395223617554\n",
      "[420/3000] loss=1.303407907485962\n",
      "[430/3000] loss=1.2960845232009888\n",
      "[440/3000] loss=1.3041883707046509\n",
      "[450/3000] loss=1.290263056755066\n",
      "[460/3000] loss=1.2812970876693726\n",
      "[470/3000] loss=1.2649893760681152\n",
      "[480/3000] loss=1.2286810874938965\n",
      "[490/3000] loss=1.240902304649353\n",
      "[500/3000] loss=1.2010998725891113\n",
      "[510/3000] loss=1.2395610809326172\n",
      "[520/3000] loss=1.1884821653366089\n",
      "[530/3000] loss=1.1885000467300415\n",
      "[540/3000] loss=1.185315728187561\n",
      "[550/3000] loss=1.201656460762024\n",
      "[560/3000] loss=1.168337345123291\n",
      "[570/3000] loss=1.2001818418502808\n",
      "[580/3000] loss=1.1690139770507812\n",
      "[590/3000] loss=1.1607533693313599\n",
      "[600/3000] loss=1.155311942100525\n",
      "[600/3000] val_loss=1.332377314567566\n",
      "[610/3000] loss=1.1197789907455444\n",
      "[620/3000] loss=1.0985994338989258\n",
      "[630/3000] loss=1.1182559728622437\n",
      "[640/3000] loss=1.117108941078186\n",
      "[650/3000] loss=1.1031017303466797\n",
      "[660/3000] loss=1.110382318496704\n",
      "[670/3000] loss=1.0985742807388306\n",
      "[680/3000] loss=1.085914969444275\n",
      "[690/3000] loss=1.0877141952514648\n",
      "[700/3000] loss=1.0589147806167603\n",
      "[710/3000] loss=1.0497560501098633\n",
      "[720/3000] loss=1.0697866678237915\n",
      "[730/3000] loss=1.0739959478378296\n",
      "[740/3000] loss=1.0264297723770142\n",
      "[750/3000] loss=1.0593396425247192\n",
      "[760/3000] loss=1.050926685333252\n",
      "[770/3000] loss=1.04985773563385\n",
      "[780/3000] loss=1.0334097146987915\n",
      "[790/3000] loss=1.0418049097061157\n",
      "[800/3000] loss=1.019450068473816\n",
      "[800/3000] val_loss=1.2770578861236572\n",
      "[810/3000] loss=1.0144649744033813\n",
      "[820/3000] loss=1.050085186958313\n",
      "[830/3000] loss=0.9900527000427246\n",
      "[840/3000] loss=0.9839951395988464\n",
      "[850/3000] loss=0.9727594256401062\n",
      "[860/3000] loss=0.9700837731361389\n",
      "[870/3000] loss=0.9561665058135986\n",
      "[880/3000] loss=0.9648270010948181\n",
      "[890/3000] loss=0.9693045020103455\n",
      "[900/3000] loss=0.9650039076805115\n",
      "[910/3000] loss=0.980502188205719\n",
      "[920/3000] loss=0.9431833624839783\n",
      "[930/3000] loss=0.9616169929504395\n",
      "[940/3000] loss=0.9655628800392151\n",
      "[950/3000] loss=0.96038818359375\n",
      "[960/3000] loss=0.9146249890327454\n",
      "[970/3000] loss=0.9145300388336182\n",
      "[980/3000] loss=0.9398558139801025\n",
      "[990/3000] loss=0.9240484833717346\n",
      "[1000/3000] loss=0.9266322255134583\n",
      "[1000/3000] val_loss=1.239262342453003\n",
      "[1010/3000] loss=0.9092643857002258\n",
      "[1020/3000] loss=0.9232390522956848\n",
      "[1030/3000] loss=0.9318823218345642\n",
      "[1040/3000] loss=0.9330024123191833\n",
      "[1050/3000] loss=0.9408157467842102\n",
      "[1060/3000] loss=0.9098221659660339\n",
      "[1070/3000] loss=0.9134228229522705\n",
      "[1080/3000] loss=0.923933207988739\n",
      "[1090/3000] loss=0.9159555435180664\n",
      "[1100/3000] loss=0.8893186450004578\n",
      "[1110/3000] loss=0.8721024990081787\n",
      "[1120/3000] loss=0.8752036690711975\n",
      "[1130/3000] loss=0.8787385821342468\n",
      "[1140/3000] loss=0.8736730217933655\n",
      "[1150/3000] loss=0.8670690655708313\n",
      "[1160/3000] loss=0.8705306053161621\n",
      "[1170/3000] loss=0.8809890151023865\n",
      "[1180/3000] loss=0.8595228791236877\n",
      "[1190/3000] loss=0.838435709476471\n",
      "[1200/3000] loss=0.8530063629150391\n",
      "[1200/3000] val_loss=1.221779704093933\n",
      "[1210/3000] loss=0.8964600563049316\n",
      "[1220/3000] loss=0.8497281074523926\n",
      "[1230/3000] loss=0.8430184721946716\n",
      "[1240/3000] loss=0.8533296585083008\n",
      "[1250/3000] loss=0.8390205502510071\n",
      "[1260/3000] loss=0.8252155184745789\n",
      "[1270/3000] loss=0.8140295147895813\n",
      "[1280/3000] loss=0.8194339275360107\n",
      "[1290/3000] loss=0.8154957890510559\n",
      "[1300/3000] loss=0.8358416557312012\n",
      "[1310/3000] loss=0.7816400527954102\n",
      "[1320/3000] loss=0.7837716937065125\n",
      "[1330/3000] loss=0.7885763645172119\n",
      "[1340/3000] loss=0.8005313873291016\n",
      "[1350/3000] loss=0.7894609570503235\n",
      "[1360/3000] loss=0.7772995829582214\n",
      "[1370/3000] loss=0.7962198257446289\n",
      "[1380/3000] loss=0.8072689175605774\n",
      "[1390/3000] loss=0.7780644297599792\n",
      "[1400/3000] loss=0.753608763217926\n",
      "[1400/3000] val_loss=1.2372260093688965\n",
      "[1410/3000] loss=0.7669021487236023\n",
      "[1420/3000] loss=0.7504308819770813\n",
      "[1430/3000] loss=0.763847827911377\n",
      "[1440/3000] loss=0.719311535358429\n",
      "[1450/3000] loss=0.7527608871459961\n",
      "[1460/3000] loss=0.7188202738761902\n",
      "[1470/3000] loss=0.7108995914459229\n",
      "[1480/3000] loss=0.7053334712982178\n",
      "[1490/3000] loss=0.7469838261604309\n",
      "[1500/3000] loss=0.728151798248291\n",
      "[1510/3000] loss=0.7442393898963928\n",
      "[1520/3000] loss=0.6902034878730774\n",
      "[1530/3000] loss=0.6873695254325867\n",
      "[1540/3000] loss=0.7075240015983582\n",
      "[1550/3000] loss=0.7030291557312012\n",
      "[1560/3000] loss=0.6619462370872498\n",
      "[1570/3000] loss=0.67987459897995\n",
      "[1580/3000] loss=0.7046979069709778\n",
      "[1590/3000] loss=0.6529315114021301\n",
      "[1600/3000] loss=0.6609196662902832\n",
      "[1600/3000] val_loss=1.2401046752929688\n",
      "[1610/3000] loss=0.6821781992912292\n",
      "[1620/3000] loss=0.6688264012336731\n",
      "[1630/3000] loss=0.6326441168785095\n",
      "[1640/3000] loss=0.6497206687927246\n",
      "[1650/3000] loss=0.6558765172958374\n",
      "[1660/3000] loss=0.648003876209259\n",
      "[1670/3000] loss=0.6534042954444885\n",
      "[1680/3000] loss=0.6148775219917297\n",
      "[1690/3000] loss=0.6326451897621155\n",
      "[1700/3000] loss=0.5945823788642883\n",
      "[1710/3000] loss=0.6146822571754456\n",
      "[1720/3000] loss=0.6211592555046082\n",
      "[1730/3000] loss=0.6190371513366699\n",
      "[1740/3000] loss=0.6102657318115234\n",
      "[1750/3000] loss=0.5980936884880066\n",
      "[1760/3000] loss=0.5938034653663635\n",
      "[1770/3000] loss=0.6025283336639404\n",
      "[1780/3000] loss=0.5541889071464539\n",
      "[1790/3000] loss=0.5542961955070496\n",
      "[1800/3000] loss=0.5781669616699219\n",
      "[1800/3000] val_loss=1.2495453357696533\n",
      "[1810/3000] loss=0.5969358086585999\n",
      "[1820/3000] loss=0.5529839396476746\n",
      "[1830/3000] loss=0.5660580992698669\n",
      "[1840/3000] loss=0.5621973872184753\n",
      "[1850/3000] loss=0.5638461112976074\n",
      "[1860/3000] loss=0.5482601523399353\n",
      "[1870/3000] loss=0.5260151624679565\n",
      "[1880/3000] loss=0.5232518315315247\n",
      "[1890/3000] loss=0.5039487481117249\n",
      "[1900/3000] loss=0.5167519450187683\n",
      "[1910/3000] loss=0.5170871615409851\n",
      "[1920/3000] loss=0.5023050904273987\n",
      "[1930/3000] loss=0.5154381394386292\n",
      "[1940/3000] loss=0.5121541619300842\n",
      "[1950/3000] loss=0.4992597997188568\n",
      "[1960/3000] loss=0.5059801936149597\n",
      "[1970/3000] loss=0.5168923735618591\n",
      "[1980/3000] loss=0.5027769207954407\n",
      "[1990/3000] loss=0.4893776476383209\n",
      "[2000/3000] loss=0.48147299885749817\n",
      "[2000/3000] val_loss=1.2932772636413574\n",
      "[2010/3000] loss=0.4718289375305176\n",
      "[2020/3000] loss=0.49185970425605774\n",
      "[2030/3000] loss=0.4660451114177704\n",
      "[2040/3000] loss=0.4948213994503021\n",
      "[2050/3000] loss=0.4556538164615631\n",
      "[2060/3000] loss=0.4776127338409424\n",
      "[2070/3000] loss=0.46699246764183044\n",
      "[2080/3000] loss=0.4699063301086426\n",
      "[2090/3000] loss=0.45365604758262634\n",
      "[2100/3000] loss=0.43929266929626465\n",
      "[2110/3000] loss=0.4413585364818573\n",
      "[2120/3000] loss=0.4166061580181122\n",
      "[2130/3000] loss=0.414776474237442\n",
      "[2140/3000] loss=0.44561767578125\n",
      "[2150/3000] loss=0.4208601415157318\n",
      "[2160/3000] loss=0.44039079546928406\n",
      "[2170/3000] loss=0.42456817626953125\n",
      "[2180/3000] loss=0.41192564368247986\n",
      "[2190/3000] loss=0.4392205774784088\n",
      "[2200/3000] loss=0.4005775451660156\n",
      "[2200/3000] val_loss=1.3461244106292725\n",
      "[2210/3000] loss=0.42763352394104004\n",
      "[2220/3000] loss=0.4015202224254608\n",
      "[2230/3000] loss=0.39384725689888\n",
      "[2240/3000] loss=0.42487654089927673\n",
      "[2250/3000] loss=0.39665690064430237\n",
      "[2260/3000] loss=0.38846340775489807\n",
      "[2270/3000] loss=0.39677342772483826\n",
      "[2280/3000] loss=0.3767363727092743\n",
      "[2290/3000] loss=0.379908949136734\n",
      "[2300/3000] loss=0.3781909644603729\n",
      "[2310/3000] loss=0.3710156977176666\n",
      "[2320/3000] loss=0.3822954595088959\n",
      "[2330/3000] loss=0.3807124197483063\n",
      "[2340/3000] loss=0.36259332299232483\n",
      "[2350/3000] loss=0.34293332695961\n",
      "[2360/3000] loss=0.35276246070861816\n",
      "[2370/3000] loss=0.36254391074180603\n",
      "[2380/3000] loss=0.36767053604125977\n",
      "[2390/3000] loss=0.368600457906723\n",
      "[2400/3000] loss=0.37061378359794617\n",
      "[2400/3000] val_loss=1.4381016492843628\n",
      "[2410/3000] loss=0.34170451760292053\n",
      "[2420/3000] loss=0.35085225105285645\n",
      "[2430/3000] loss=0.3374137878417969\n",
      "[2440/3000] loss=0.3497389853000641\n",
      "[2450/3000] loss=0.34841421246528625\n",
      "[2460/3000] loss=0.336627334356308\n",
      "[2470/3000] loss=0.32507574558258057\n",
      "[2480/3000] loss=0.33339962363243103\n",
      "[2490/3000] loss=0.32753291726112366\n",
      "[2500/3000] loss=0.3207460939884186\n",
      "[2510/3000] loss=0.3176422417163849\n",
      "[2520/3000] loss=0.33238959312438965\n",
      "[2530/3000] loss=0.3099023103713989\n",
      "[2540/3000] loss=0.3266519606113434\n",
      "[2550/3000] loss=0.3241663873195648\n",
      "[2560/3000] loss=0.3155349791049957\n",
      "[2570/3000] loss=0.3089463710784912\n",
      "[2580/3000] loss=0.3055820167064667\n",
      "[2590/3000] loss=0.31398892402648926\n",
      "[2600/3000] loss=0.3163107931613922\n",
      "[2600/3000] val_loss=1.4727866649627686\n",
      "[2610/3000] loss=0.29320594668388367\n",
      "[2620/3000] loss=0.30857786536216736\n",
      "[2630/3000] loss=0.30602335929870605\n",
      "[2640/3000] loss=0.30093660950660706\n",
      "[2650/3000] loss=0.2987664043903351\n",
      "[2660/3000] loss=0.29416534304618835\n",
      "[2670/3000] loss=0.28909018635749817\n",
      "[2680/3000] loss=0.2941285967826843\n",
      "[2690/3000] loss=0.30046796798706055\n",
      "[2700/3000] loss=0.2932129502296448\n",
      "[2710/3000] loss=0.3014451563358307\n",
      "[2720/3000] loss=0.2865621745586395\n",
      "[2730/3000] loss=0.2918984889984131\n",
      "[2740/3000] loss=0.26967495679855347\n",
      "[2750/3000] loss=0.27184513211250305\n",
      "[2760/3000] loss=0.28793445229530334\n",
      "[2770/3000] loss=0.29259130358695984\n",
      "[2780/3000] loss=0.26137682795524597\n",
      "[2790/3000] loss=0.2769660949707031\n",
      "[2800/3000] loss=0.26941514015197754\n",
      "[2800/3000] val_loss=1.5064588785171509\n",
      "[2810/3000] loss=0.26696550846099854\n",
      "[2820/3000] loss=0.2575717270374298\n",
      "[2830/3000] loss=0.28139859437942505\n",
      "[2840/3000] loss=0.24609416723251343\n",
      "[2850/3000] loss=0.2610081136226654\n",
      "[2860/3000] loss=0.26770126819610596\n",
      "[2870/3000] loss=0.26797184348106384\n",
      "[2880/3000] loss=0.2731340825557709\n",
      "[2890/3000] loss=0.2675516903400421\n",
      "[2900/3000] loss=0.25362858176231384\n",
      "[2910/3000] loss=0.2630058228969574\n",
      "[2920/3000] loss=0.2619657814502716\n",
      "[2930/3000] loss=0.2563728392124176\n",
      "[2940/3000] loss=0.24908702075481415\n",
      "[2950/3000] loss=0.24515052139759064\n",
      "[2960/3000] loss=0.2449071854352951\n",
      "[2970/3000] loss=0.25170284509658813\n",
      "[2980/3000] loss=0.26049506664276123\n",
      "[2990/3000] loss=0.25521889328956604\n",
      "training took 951.7507138252258 seconds\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 200 # validate model every .. iterations\n",
    "log_interval = 10 # log training loss every .. iterations\n",
    "eval_iters = 20 # number of batches for loss estimation\n",
    "gradient_accumulation_steps = 5 # used to simulate larger training batch sizes\n",
    "batch_size = 96 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_size = 128 # sequence length\n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' # vocabulary\n",
    "vocab_size = len(vocab) # 40\n",
    "n_layer = 8 # number of layers\n",
    "n_head = 8 # number of attention heads\n",
    "hidden_size = 256 # layer size\n",
    "dropout = 0.1 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "learning_rate = 7e-4 # max learning rate\n",
    "max_iters = 3000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9 # for AdamW\n",
    "beta2 = 0.999 # for AdamW\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable with 0.0\n",
    "warmup_iters = 200 # how many steps to warm up for\n",
    "min_lr = 7e-5 # minimum learning rate, usually ~= learning_rate/10\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def load_data(split):\n",
    "    import re\n",
    "    \n",
    "    with open(f'trump_{split}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = text.lower() # convert to lower case\n",
    "    text = re.sub('[^a-z0-9 .!?]', ' ', text) # replace all unknown chars with ' '\n",
    "    text = re.sub(' +', ' ', text) # reduce multiple blanks to one\n",
    "    text = [vocab.index(t) for t in text]\n",
    "    text = torch.tensor(text, dtype=torch.long, device=DEVICE)\n",
    "    return text\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Random starting indices (shape: [batch_size])\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,), device=DEVICE)\n",
    "\n",
    "    # Create a 2D index tensor of shape batch_size X context_size\n",
    "    #  For each element in ix, we want to collect [i, i+1, ..., i+context_size-1].\n",
    "    #  So we broadcast-add a range of length `context_size` to each element of ix.\n",
    "    x_positions = ix.unsqueeze(-1) + torch.arange(context_size, device=DEVICE)\n",
    "    y_positions = x_positions + 1  # Shift by 1\n",
    "    x = data[x_positions]  # batch_size X context_size\n",
    "    y = data[y_positions]  # batch_size X context_size\n",
    "\n",
    "    return x, y\n",
    "    # old function\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# data, model, optimizer, etc. \n",
    "train_data = load_data('train')\n",
    "val_data = load_data('val')\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(X, Y)\n",
    "    loss.backward()\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    if iter_num % log_interval == 0:\n",
    "        print(f'[{iter_num}/{max_iters}] loss={loss.item()}')\n",
    "    if iter_num % eval_interval == 0:\n",
    "        val_loss = estimate_loss()['val']\n",
    "        print(f'[{iter_num}/{max_iters}] val_loss={val_loss}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    X, Y = get_batch('train')\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "print(f'training took {time.time()-t0} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f0e3c",
   "metadata": {},
   "source": [
    "## Exercise 8: Inference\n",
    "\n",
    "Add a method `generate` to the class `GPT` that takes arguments `x, max_new_tokens, temperature=1.0`. \n",
    "The method should take a batch of token sequences `x`, which it should extend by `max_new_tokens` new tokens generated by the model. \n",
    "Once you have computed the logits for the next token, divide them by `temperature` before applying the softmax. \n",
    "After applying the softmax, sample the next token from the resulting categorical distribution. \n",
    "Try out different values for `temperature` and compare the results to those from the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595e921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, x, max_new_tokens, temperature=1.0):\n",
    "    self.eval()\n",
    "    N, T = x.size()\n",
    "    output = torch.zeros(size=(N, T + max_new_tokens), dtype=torch.long, device=x.device)\n",
    "    output[:, :T] = x\n",
    "    for t in range(T, T + max_new_tokens):\n",
    "        logits = self(output[:, t-context_size:t], None)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        output[:, t] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "    return output\n",
    "\n",
    "GPT.generate = generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c229c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/xfwm3xr90019fcl3by7y113c0000gn/T/ipykernel_74987/1450387794.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- temperature=0.1 ---\n",
      "                                                                                                              make america great with me the was the best in the world. but i said the other day i want to thank my family because i want to thank the banks. i want to thank the banks. the only really want to be that the thing that happen. i want to thank that s the way. i want to thank that the best messengers when i want to thank the polls. the only thing that we re going to make some really good to meant for them. we re going to be a famiture. we re going to be a great with mexico. we re going to be a very very successful p\n",
      "--- temperature=0.2 ---\n",
      "                                                                                                              make america great with me in free. but i don t know what the hell his to have the coment of the greatest achect of the country. the other candidence and they re going to go to there four things. and they re going to go to there for the lost. they re going to go to there for the folks. they re going to go to there. they re going to go to there for the folks. they re going to go to there. they re going to go through they re going to go to their money for the wall. they re going to go to there. they re going to go \n",
      "--- temperature=0.3 ---\n",
      "                                                                                                              make america great if i would be there will be totally correcy with me. i don t know what they re doing where they re not going to do their job but they re not going to let them up. they re not going to do their job. they re not going to do their job. they re not going to stop it. they re going to be a thing that s going to be very soon. i m not going to leave. i m nothing some of the other candidates and they re not going to leave the problems. they re going to change they re going to do their job. and they re g\n",
      "--- temperature=0.5 ---\n",
      "                                                                                                              make america great an in frantas. i want to thank my stance and that s what the really happening to us the become that i ll say i m not going to win. i m not going to happy. i m not going to happy i m going to be not elected. and i m not going to happy come into the other candidates and they re going to go through the one that have the most incredible they re going to be nothing way. we re going to have it because i would have never been there back to make them for a like a moment folks. i have to stop it. i have\n",
      "--- temperature=0.6 ---\n",
      "                                                                                                              make america greater. but if they don t make the money but i said to myself i m seet what s happening. i m not going to spend you million okay? i mean money i can t go in my ran. so i say don t know. that i say what i can t mean i m a protester. i m a very good place. i have a big problem. i have to get along with china. i have problems that i had the background they have a friend of mine and they re going to know they re going to let them up and the guy ve goten to make a million so i ll tell you what said i wou\n",
      "--- temperature=0.7 ---\n",
      "                                                                                                              make america greater. i ve got to see what s going to do. i don t know who that we don t know who they re all this. we re going to get worse than they colect in on they showed up to my family. so she said you know what? i saw that? you know? i was saying them young. you had think the banks in the world. and they make the one deduction like meeting he s done an endorsement. but he s a fringest phony and they re big even that. right? what means i would have had so much hands. and they take our jobs and every thing \n",
      "--- temperature=0.8 ---\n",
      "                                                                                                              make america great just every singling me with me in a very very omen number. but i have a clue. i have to tell you. i won t have to be through. if you ve got to take you. you see this the one that s going on. i think so if you saw take a lot easteday. could you know. i m wrong in iowa because what a little bit tale and big like nice and saying i m talking about a but marginalizing trade but i aslamic television. he saw i can t let along forever. and i think it s going to take how because they shouldn t get the m\n",
      "--- temperature=1.0 ---\n",
      "                                                                                                              make america greater. but those problems that could be the most less than ou miners will be oblist the best the deal. but sort? it actually believe jy becausecieive that s groups at these night. we can ever know ever things nobody be after you. they re going to kind oh things that are going to be up the obama. but i m going to do so paying tax to and loon over senate what happened in san francisco what his frankly s incrasse in indiana the greatest making the questions but we re going to started and we re going t\n"
     ]
    }
   ],
   "source": [
    "temps = [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "\n",
    "for temp in temps:\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    sentence = 'make america great'\n",
    "    x = torch.ones(size=(1, context_size), dtype=torch.long, device=DEVICE) * vocab.index(' ')\n",
    "    x[:, -len(sentence):] = torch.tensor([vocab.index(c) for c in sentence], dtype=torch.long).unsqueeze(0)\n",
    "    output = model.generate(x, 500, temperature=temp)\n",
    "    text = ''.join([vocab[i] for i in output[0].cpu().numpy()])\n",
    "\n",
    "    print(f'--- temperature={temp} ---')\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30415e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
