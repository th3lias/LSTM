{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e9d729",
   "metadata": {},
   "source": [
    "# Assignment 4: Training Transformers in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment we will implement and train a small transformer model and compare it to the LSTM in the previous assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089693e",
   "metadata": {},
   "source": [
    "## Exercise 1: Causal Self-Attention\n",
    "\n",
    "Write a class named `CausalSelfAttention` that derives from `nn.Module` and whose `__init__` method takes (apart from the trivial `self`) one argument `hidden_size`. Implement a method `forward` that takes an input sequence `x` of shape $(N, T, D)$ (where $N$ is batch size, $T$ is sequence length, $D$ is hidden size) and performs scaled dot-product self-attention, i.e., \n",
    "$$\n",
    "Y = \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{D}} Q K^\\top\\right) V,\n",
    "$$\n",
    "where $Q = X W_Q$ and $K = X W_K$ and $V = X W_V$ and $X \\in \\mathbb{R}^{T \\times D}$ and $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$ and softmax is applied in a row-wise manner and neglecting bias units. \n",
    "It is called self-attention because $Q, K, V$ are all computed from the same input $X$, which hence attends to itself. \n",
    "\n",
    "To have the attention be *causal* we need to make sure that we do not allow peeks into the future. That is, the output at time $t$ must be a function of the input at times $1, \\dots, t$ but no further. The score matrix $E = \\frac{1}{\\sqrt{D}} Q K^\\top$ has a shape of $T \\times T$ and the entry $e_{ij}$ measures how strong the query at time $i$ attends to the key at time $j$. Therefore, positions where $j > i$ constitute peeks into the future and we have to set the corresponding attention values (i.e., the softmax-activated score) to zero. We can do that by setting the corresponding score to `float('-inf')`, which has the advantage that the normalization is adjusted automatically by the softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28afb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.WQ = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.WK = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.WV = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)                                                  #  (N, T, D)\n",
    "        K = self.WK(x)                                                  #  (N, T, D)\n",
    "        V = self.WV(x)                                                  #  (N, T, D)\n",
    "        Q = Q / math.sqrt(D)\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2))                     #  (N, T, D) * (N, D, T) -> (N, T, T)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)    #  (T, T)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))           #  (N, T, T)\n",
    "        attention = F.softmax(scores, dim=-1)                           #  (N, T, T)\n",
    "        out = torch.matmul(attention, V)                                #  (N, T, T) * (N, T, D) -> (N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ee970",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Write a class `MultiHeadCausalSelfAttention` that derives from `nn.Module` and extends the functionality of `CausalSelfAttention` from the previous exercise. \n",
    "The `__init__` method takes arguments `hidden_size, n_head, dropout`. `n_head` specifies the number of attention heads and `dropout` specifies the intensity for the dropout layers. \n",
    "The `forward` method should split the hidden dimension of the pre-activations (i.e., $Q, K, V$) in `n_head` equally sized parts and perform attention to these parts in parallel. \n",
    "Apply the first dropout layer direcly after the softmax. \n",
    "After the multiplication of the scores with the values, recombine the output of the distinct attention heads back into a single hidden dimension of size $D$, i.e., the resulting shape should be the shape of the input. \n",
    "Then perform an additional output projection again resulting in a hidden dimension of $D$. \n",
    "Finally, apply the second dropout layer after the output projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddee2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(MultiHeadCausalSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.WQ = nn.Linear(hidden_size, hidden_size)\n",
    "        self.WK = nn.Linear(hidden_size, hidden_size)\n",
    "        self.WV = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.size()\n",
    "        Q = self.WQ(x)\n",
    "        K = self.WK(x)\n",
    "        V = self.WV(x)\n",
    "        Q = Q.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        K = K.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        V = V.view(N, T, self.n_head, D // self.n_head).permute(0, 2, 1, 3)\n",
    "        Q = Q / math.sqrt(D // self.n_head)\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)\n",
    "        scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(self.dropout(attention), V).permute(0, 2, 1, 3).contiguous().view(N, T, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b329d",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Layer Perceptron\n",
    "\n",
    "Write a class `MLP` that derives from `nn.Module` and whose `__init__` method takes two arguments: `hidden_size` and `dropout`. \n",
    "It should implement a 2-layer feedforward network with `hidden_size` inputs, `4*hidden_size` hiddens, and `hidden_size` outputs. \n",
    "It should apply the GELU activation function to the hiddens and dropout to the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca16758",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W1 = nn.Linear(hidden_size, 4*hidden_size)\n",
    "        self.W2 = nn.Linear(4*hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.W2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645751",
   "metadata": {},
   "source": [
    "## Exercise 4: Block\n",
    "\n",
    "Write a class `Block` that derives from `nn.Module` and whose `__init__` method takes arguments `hidden_size, n_head, dropout`. \n",
    "It should apply `nn.LayerNorm`, `CausalMultiHeadSelfAttention`, `nn.LayerNorm`, `MLP` in that order and feature residual connections from the input to the output of `CausalMultiHeadSelfAttention` and from there to the output of `MLP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.attention = MultiHeadCausalSelfAttention(hidden_size, n_head, dropout)\n",
    "        self.mlp = MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(self.ln1(x)) + x\n",
    "        x = self.mlp(self.ln2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b7022",
   "metadata": {},
   "source": [
    "## Exercise 5: GPT\n",
    "\n",
    "Write a class `GPT` that derives from `nn.Module` and whose `__init__` method takes arguments `vocab_size, context_size, hidden_size, n_layer, n_head, dropout`. \n",
    "The `forward` method should take two arguments `x, y` representing sequences of input and target tokens, respectively, both of which have type `torch.long` and shape ($N$, $T$), and returns logits and loss as a tuple. \n",
    "The `GPT` module should feature two `nn.Embedding` layers, one for token embeddings and one for positional embedding, i.e., it should embed the position of the corresponding token within the input sequence. \n",
    "The positional embedding is necessary for the Transformer to determine the order of its inputs. \n",
    "Add the two embeddings and apply a dropout layer. \n",
    "Next, apply `n_layers` layers of `Block`s followed by a `nn.LayerNorm` and a `nn.Linear` (without bias) mapping to an output dimension of `vocab_size`. \n",
    "Finally, apply the cross-entropy loss function to the logits. \n",
    "To save some parameters, apply weight tying between the token embedding layer and the output layer, i.e., they should use the same weights. \n",
    "Initialize all weights using a normal distribution with a mean of zero and a standard deviation of 0.02 (except for the output layers of the `MLP`s use $0.02/\\sqrt{2 * \\mathtt{n\\_layer}}$) and all biases to zero. \n",
    "Use the argument `dropout` as intensity for all dropout layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe14a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout=0.0):\n",
    "        super(GPT, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(context_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.linear.weight = self.token_embedding.weight  # weight tying\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                torch.nn.init.ones_(module.weight)\n",
    "        torch.nn.init.normal_(self.linear.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        N, T = x.size()\n",
    "        x = self.token_embedding(x) + self.position_embedding(torch.arange(T, device=x.device))\n",
    "        x = self.dropout(x)\n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.linear(x)\n",
    "        logits = x\n",
    "        if y is None:\n",
    "            return logits\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc12d6",
   "metadata": {},
   "source": [
    "## Exercise 6: Optimizer\n",
    "\n",
    "Add a method `configure_optimizers` to the class `GPT` that takes arguments `weight_decay, learning_rate, betas`. \n",
    "Divide the model parameters into two groups. \n",
    "The first group consists of all parameters with at least 2 dimensions, e.g., weight/embedding matrices and uses a decay of `weight_decay`. \n",
    "The second group consists of all other parameters, e.g., biases and layer norms, and does not use weight decay.\n",
    "Construct and return a `torch.optim.AdamW` optimizer with `learning_rate` and `betas` that operates on these two parameter groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e8be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "    at_least_2_dim_params = (p for p in self.parameters() if p.ndimension() >= 2)\n",
    "    other_params = (p for p in self.parameters() if p.ndimension() < 2)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {'params': at_least_2_dim_params, 'weight_decay': weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0}\n",
    "            ], lr=learning_rate, betas=betas)\n",
    "    return optimizer\n",
    "\n",
    "GPT.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b472",
   "metadata": {},
   "source": [
    "## Exercise 7: Training\n",
    "\n",
    "In the code cell below you find some globals, helper functions, and boilerplate code. Extend the given code by a training loop that \n",
    "* stops after `max_iters` iterations\n",
    "* applies the learning rate schedule implemented in `get_lr`\n",
    "* applies gradient clipping at `grad_clip` using `torch.nn.utils.clip_grad_norm_`\n",
    "* accumulates gradients for `gradient_accumulation_steps` batches before each weight update\n",
    "* logs the training loss and learning rate every `log_interval` iterations\n",
    "* evaluates (and potentially checkpoints) the model using `estimate_loss` every `eval_iters` iterations.\n",
    "\n",
    "The provided hyperparameter values should be a good guess for training a tiny model on CPU but feel free to experiment with them as you please. In particular, if you have a GPU available, you can try to scale things up a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641a76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2000] loss=3.7075612545013428\n",
      "[0/2000] val_loss=3.385730743408203\n",
      "[10/2000] loss=3.265213966369629\n",
      "[20/2000] loss=3.1491892337799072\n",
      "[30/2000] loss=3.015291452407837\n",
      "[40/2000] loss=2.955625295639038\n",
      "[50/2000] loss=2.848778486251831\n",
      "[60/2000] loss=2.728976011276245\n",
      "[70/2000] loss=2.647765874862671\n",
      "[80/2000] loss=2.610205888748169\n",
      "[90/2000] loss=2.5773584842681885\n",
      "[100/2000] loss=2.5336906909942627\n",
      "[110/2000] loss=2.451822280883789\n",
      "[120/2000] loss=2.4977848529815674\n",
      "[130/2000] loss=2.3381283283233643\n",
      "[140/2000] loss=2.3474864959716797\n",
      "[150/2000] loss=2.414625883102417\n",
      "[160/2000] loss=2.3886094093322754\n",
      "[170/2000] loss=2.382488489151001\n",
      "[180/2000] loss=2.422766923904419\n",
      "[190/2000] loss=2.364013433456421\n",
      "[200/2000] loss=2.345000982284546\n",
      "[210/2000] loss=2.312162160873413\n",
      "[220/2000] loss=2.322686195373535\n",
      "[230/2000] loss=2.3351938724517822\n",
      "[240/2000] loss=2.3693125247955322\n",
      "[250/2000] loss=2.283604383468628\n",
      "[250/2000] val_loss=2.3825345039367676\n",
      "[260/2000] loss=2.281049966812134\n",
      "[270/2000] loss=2.4159021377563477\n",
      "[280/2000] loss=2.223057746887207\n",
      "[290/2000] loss=2.324338912963867\n",
      "[300/2000] loss=2.243927240371704\n",
      "[310/2000] loss=2.305330753326416\n",
      "[320/2000] loss=2.25174617767334\n",
      "[330/2000] loss=2.3450934886932373\n",
      "[340/2000] loss=2.2647182941436768\n",
      "[350/2000] loss=2.296757698059082\n",
      "[360/2000] loss=2.3752243518829346\n",
      "[370/2000] loss=2.2727012634277344\n",
      "[380/2000] loss=2.16326642036438\n",
      "[390/2000] loss=2.2339260578155518\n",
      "[400/2000] loss=2.16398024559021\n",
      "[410/2000] loss=2.126765012741089\n",
      "[420/2000] loss=2.222280263900757\n",
      "[430/2000] loss=2.1212708950042725\n",
      "[440/2000] loss=2.082479953765869\n",
      "[450/2000] loss=2.091336488723755\n",
      "[460/2000] loss=2.008716106414795\n",
      "[470/2000] loss=2.033496141433716\n",
      "[480/2000] loss=2.109118938446045\n",
      "[490/2000] loss=2.1249918937683105\n",
      "[500/2000] loss=2.0079991817474365\n",
      "[500/2000] val_loss=2.176757574081421\n",
      "[510/2000] loss=2.1701865196228027\n",
      "[520/2000] loss=2.0716147422790527\n",
      "[530/2000] loss=1.9256163835525513\n",
      "[540/2000] loss=1.983504295349121\n",
      "[550/2000] loss=2.0188491344451904\n",
      "[560/2000] loss=2.0379726886749268\n",
      "[570/2000] loss=2.0877556800842285\n",
      "[580/2000] loss=2.1008543968200684\n",
      "[590/2000] loss=1.9327144622802734\n",
      "[600/2000] loss=1.918721318244934\n",
      "[610/2000] loss=1.9633556604385376\n",
      "[620/2000] loss=2.0004093647003174\n",
      "[630/2000] loss=1.8441036939620972\n",
      "[640/2000] loss=1.8603607416152954\n",
      "[650/2000] loss=1.7892670631408691\n",
      "[660/2000] loss=1.8604928255081177\n",
      "[670/2000] loss=1.9805485010147095\n",
      "[680/2000] loss=1.8659648895263672\n",
      "[690/2000] loss=1.7887548208236694\n",
      "[700/2000] loss=1.9036785364151\n",
      "[710/2000] loss=1.8470481634140015\n",
      "[720/2000] loss=1.9069852828979492\n",
      "[730/2000] loss=1.7530258893966675\n",
      "[740/2000] loss=1.8146556615829468\n",
      "[750/2000] loss=1.8335531949996948\n",
      "[750/2000] val_loss=1.962272047996521\n",
      "[760/2000] loss=1.772656798362732\n",
      "[770/2000] loss=1.8418521881103516\n",
      "[780/2000] loss=1.7694830894470215\n",
      "[790/2000] loss=1.8196450471878052\n",
      "[800/2000] loss=1.8345125913619995\n",
      "[810/2000] loss=1.7971863746643066\n",
      "[820/2000] loss=1.8215985298156738\n",
      "[830/2000] loss=1.8604668378829956\n",
      "[840/2000] loss=1.6448079347610474\n",
      "[850/2000] loss=1.8290868997573853\n",
      "[860/2000] loss=1.7198799848556519\n",
      "[870/2000] loss=1.742775559425354\n",
      "[880/2000] loss=1.6134880781173706\n",
      "[890/2000] loss=1.9296377897262573\n",
      "[900/2000] loss=1.6890764236450195\n",
      "[910/2000] loss=1.9246654510498047\n",
      "[920/2000] loss=1.8355423212051392\n",
      "[930/2000] loss=1.7022820711135864\n",
      "[940/2000] loss=1.7214933633804321\n",
      "[950/2000] loss=1.6484017372131348\n",
      "[960/2000] loss=1.638813853263855\n",
      "[970/2000] loss=1.6526228189468384\n",
      "[980/2000] loss=1.7428456544876099\n",
      "[990/2000] loss=1.606143832206726\n",
      "[1000/2000] loss=1.6318002939224243\n",
      "[1000/2000] val_loss=1.8352196216583252\n",
      "[1010/2000] loss=1.5811649560928345\n",
      "[1020/2000] loss=1.6641913652420044\n",
      "[1030/2000] loss=1.6417036056518555\n",
      "[1040/2000] loss=1.6328428983688354\n",
      "[1050/2000] loss=1.6754356622695923\n",
      "[1060/2000] loss=1.573818564414978\n",
      "[1070/2000] loss=1.618836760520935\n",
      "[1080/2000] loss=1.7157527208328247\n",
      "[1090/2000] loss=1.5692272186279297\n",
      "[1100/2000] loss=1.603590965270996\n",
      "[1110/2000] loss=1.5668636560440063\n",
      "[1120/2000] loss=1.6464577913284302\n",
      "[1130/2000] loss=1.5528340339660645\n",
      "[1140/2000] loss=1.7235912084579468\n",
      "[1150/2000] loss=1.7270928621292114\n",
      "[1160/2000] loss=1.5839968919754028\n",
      "[1170/2000] loss=1.6985899209976196\n",
      "[1180/2000] loss=1.571492314338684\n",
      "[1190/2000] loss=1.593698501586914\n",
      "[1200/2000] loss=1.6113395690917969\n",
      "[1210/2000] loss=1.5524619817733765\n",
      "[1220/2000] loss=1.6679891347885132\n",
      "[1230/2000] loss=1.5732418298721313\n",
      "[1240/2000] loss=1.5641461610794067\n",
      "[1250/2000] loss=1.5398368835449219\n",
      "[1250/2000] val_loss=1.7189276218414307\n",
      "[1260/2000] loss=1.5640760660171509\n",
      "[1270/2000] loss=1.4865938425064087\n",
      "[1280/2000] loss=1.577947974205017\n",
      "[1290/2000] loss=1.5686615705490112\n",
      "[1300/2000] loss=1.4610997438430786\n",
      "[1310/2000] loss=1.5218156576156616\n",
      "[1320/2000] loss=1.7546143531799316\n",
      "[1330/2000] loss=1.587797999382019\n",
      "[1340/2000] loss=1.6122866868972778\n",
      "[1350/2000] loss=1.4402899742126465\n",
      "[1360/2000] loss=1.4262871742248535\n",
      "[1370/2000] loss=1.584635853767395\n",
      "[1380/2000] loss=1.6018308401107788\n",
      "[1390/2000] loss=1.5188239812850952\n",
      "[1400/2000] loss=1.5697765350341797\n",
      "[1410/2000] loss=1.4609665870666504\n",
      "[1420/2000] loss=1.4669626951217651\n",
      "[1430/2000] loss=1.6416969299316406\n",
      "[1440/2000] loss=1.4739922285079956\n",
      "[1450/2000] loss=1.4428411722183228\n",
      "[1460/2000] loss=1.4492383003234863\n",
      "[1470/2000] loss=1.4292062520980835\n",
      "[1480/2000] loss=1.4617003202438354\n",
      "[1490/2000] loss=1.4711512327194214\n",
      "[1500/2000] loss=1.5222426652908325\n",
      "[1500/2000] val_loss=1.6339561939239502\n",
      "[1510/2000] loss=1.4389952421188354\n",
      "[1520/2000] loss=1.5356096029281616\n",
      "[1530/2000] loss=1.598990559577942\n",
      "[1540/2000] loss=1.3789548873901367\n",
      "[1550/2000] loss=1.4921427965164185\n",
      "[1560/2000] loss=1.524814486503601\n",
      "[1570/2000] loss=1.5531541109085083\n",
      "[1580/2000] loss=1.5520647764205933\n",
      "[1590/2000] loss=1.3718317747116089\n",
      "[1600/2000] loss=1.4702543020248413\n",
      "[1610/2000] loss=1.550892949104309\n",
      "[1620/2000] loss=1.4276903867721558\n",
      "[1630/2000] loss=1.3996315002441406\n",
      "[1640/2000] loss=1.364248275756836\n",
      "[1650/2000] loss=1.5026644468307495\n",
      "[1660/2000] loss=1.5224665403366089\n",
      "[1670/2000] loss=1.550089955329895\n",
      "[1680/2000] loss=1.4109209775924683\n",
      "[1690/2000] loss=1.2856227159500122\n",
      "[1700/2000] loss=1.4097723960876465\n",
      "[1710/2000] loss=1.381271243095398\n",
      "[1720/2000] loss=1.546086311340332\n",
      "[1730/2000] loss=1.380393147468567\n",
      "[1740/2000] loss=1.4879544973373413\n",
      "[1750/2000] loss=1.3537105321884155\n",
      "[1750/2000] val_loss=1.62713623046875\n",
      "[1760/2000] loss=1.457081913948059\n",
      "[1770/2000] loss=1.3384567499160767\n",
      "[1780/2000] loss=1.3702329397201538\n",
      "[1790/2000] loss=1.4569109678268433\n",
      "[1800/2000] loss=1.470110535621643\n",
      "[1810/2000] loss=1.417368769645691\n",
      "[1820/2000] loss=1.3911088705062866\n",
      "[1830/2000] loss=1.5140432119369507\n",
      "[1840/2000] loss=1.5377219915390015\n",
      "[1850/2000] loss=1.2663360834121704\n",
      "[1860/2000] loss=1.4290179014205933\n",
      "[1870/2000] loss=1.3210241794586182\n",
      "[1880/2000] loss=1.4602593183517456\n",
      "[1890/2000] loss=1.3875302076339722\n",
      "[1900/2000] loss=1.405455231666565\n",
      "[1910/2000] loss=1.3673938512802124\n",
      "[1920/2000] loss=1.3217482566833496\n",
      "[1930/2000] loss=1.3791460990905762\n",
      "[1940/2000] loss=1.3032867908477783\n",
      "[1950/2000] loss=1.3591885566711426\n",
      "[1960/2000] loss=1.3525279760360718\n",
      "[1970/2000] loss=1.3658385276794434\n",
      "[1980/2000] loss=1.4102778434753418\n",
      "[1990/2000] loss=1.3421393632888794\n",
      "training took 30.058200120925903 seconds\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 250 # validate model every .. iterations\n",
    "log_interval = 10 # log training loss every .. iterations\n",
    "eval_iters = 20 # number of batches for loss estimation\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger training batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_size = 64 # sequence length\n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' # vocabulary\n",
    "vocab_size = len(vocab) # 40\n",
    "n_layer = 4 # number of layers\n",
    "n_head = 4 # number of attention heads\n",
    "hidden_size = 128 # layer size\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9 # for AdamW\n",
    "beta2 = 0.99 # for AdamW\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable with 0.0\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "min_lr = 1e-4 # minimum learning rate, usually ~= learning_rate/10\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def load_data(split):\n",
    "    import re\n",
    "    \n",
    "    with open(f'trump_{split}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = text.lower() # convert to lower case\n",
    "    text = re.sub('[^a-z0-9 .!?]', ' ', text) # replace all unknown chars with ' '\n",
    "    text = re.sub(' +', ' ', text) # reduce multiple blanks to one\n",
    "    text = [vocab.index(t) for t in text]\n",
    "    text = torch.tensor(text, dtype=torch.long)\n",
    "    return text\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# data, model, optimizer, etc. \n",
    "train_data = load_data('train')\n",
    "val_data = load_data('val')\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(X, Y)\n",
    "    loss.backward()\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    if iter_num % log_interval == 0:\n",
    "        print(f'[{iter_num}/{max_iters}] loss={loss.item()}')\n",
    "    if iter_num % eval_interval == 0:\n",
    "        val_loss = estimate_loss()['val']\n",
    "        print(f'[{iter_num}/{max_iters}] val_loss={val_loss}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    X, Y = get_batch('train')\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "print(f'training took {time.time()-t0} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f0e3c",
   "metadata": {},
   "source": [
    "## Exercise 8: Inference\n",
    "\n",
    "Add a method `generate` to the class `GPT` that takes arguments `x, max_new_tokens, temperature=1.0`. \n",
    "The method should take a batch of token sequences `x`, which it should extend by `max_new_tokens` new tokens generated by the model. \n",
    "Once you have computed the logits for the next token, divide them by `temperature` before applying the softmax. \n",
    "After applying the softmax, sample the next token from the resulting categorical distribution. \n",
    "Try out different values for `temperature` and compare the results to those from the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595e921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, x, max_new_tokens, temperature=1.0):\n",
    "    self.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(x, torch.zeros_like(x))[0]\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x = torch.cat([x, torch.multinomial(probs, 1)], dim=-1)\n",
    "\n",
    "GPT.generate = generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c504f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/xfwm3xr90019fcl3by7y113c0000gn/T/ipykernel_14254/196454666.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature=0.2:\n",
      "make america great and they re saying the say we re going to be the saying that s of the the s going to be a back and they re going to be the saying the was the s going to be the saying the they re going to be a back the the say were with was they re going to be a lot a great the the say the s a probly the s the going to be the say we re going to be the said a the was the s going to be the things they re going to be a lot of the the say with the s that s but the was they re going to be the the say were and they re going to be the way re going to be a back the the way want to the say the will the the s maying they re going to be a great of the people the way want to the was the good the say we re going to be the said the s a lett the the a country in the say we re going to be the s going to be the saying the was the the s going to be a because the s maying the the really so the s because and they re going to be the the s going to be a because of the but the say we re going to be the way want to the say what is the s the s mayin\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=0.4:\n",
      "make america great energy country the say may in the never of our of mor for better and this was the regoing to like of big to be the ban the book and they re going to that make so with a was they we re going to be going to get because we with the re going to be the problem. i say we have a great hould and they re going to let a go not the we re going to be the got the the great of people the amany and of the really what that s going to be a lot the signe they re going to the say lot that it s a some for the cars of we have to be the go the poy with the the was a the going to the want we re going to be a big to be a probly because i said a may a lot the probly in the way re s going to me going to make a be a greate because of the really of the s every the s winning the the re going to scome don t be a the the one the one the s so the way they have the s going to the say want the was the s come in with our country every seen some and i said with was that they have the s so many of the s she s going to happen and the say the hav\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=0.6:\n",
      "make america great of the say me just the regan of things they had it s going to new they re going to keep. and we have great and you and they re can t a frirst and affice the s think they re not going to do there ve of the came pressident because of in the one s a mexico the a borders and so the s maying it let us of a criding thing buy and he greatest will they because bot do think they re going to the really are that s going to in our because i m going to our great of i don t not thank in the mact segrate. but and but so we ve going to that at because i m that s said a if the way they we re going to that and say i was a s the s going to we re going to be the s say re going to leading to the dishe it. they re going. they re many ou going to start of this again to do a there of take the countries the s longer what you know i don t the may good happened they do they ve gotter and they re going to be you know she going to it. i don t the it that s with the the to dish one things they re going to go be good a my win a mayher ha \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=0.7:\n",
      "make america great of think they warould. i they say want the to give take a uned of teer if thinks we have s a reains. they never of the souble and i said and with and we there man 1000. it s you country. they re going to there can any re really she great supposid of the good proobly what s something to support is busive the s the indoestandion do new whith themen the happane they re no guver and they her he way they s been the bridger. i cobaming of with s me. i m nike someth people in come and and a some companies and everything. we us going to have and everybody they had i was trudget and they didn t win the can a because bing it think up a because ubsestant and he don t go to baback. it win it happens. we re going to do the same probably the have s is million that all get the how could that happens t the believe. we re going to say we re muchan we its saying it we have see want and we re guy re going to nemexing it. they don. they that s done happent and they re s going to do say hable and they re going to have the heat s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=0.8:\n",
      "make america great of nom now go be to supported of things 10 bout is things the the s care so what they re going to be up of right going to a think may wimm gus this everybody her s cauotals said the do said guy. it i tell ress the red country. most but wouldn t to setrennt and there work. i we that s the very want that s 18 2 6 year a great of business the care countings. we have of band that me our of the being that hourd america securing about tellous and he would to bring to the believe to be a gave a clinton this let much the let that the brough they that and of the respersifent cartly. the are not get on may think and we have to many the can bud want chomin the will agains the the othing so many we re negant drisht. we what they said weable reading we re going to go save nuctionally and in say i said to mike a was lele problers and the came about me fornk ever bad lot lamer because a fort on the hold. i said now. we lone they he don t they don t the not s not to s a kit bry clintons maybe mising and they like just rup a\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=0.9:\n",
      "make america great seent. i don t work. you 6don t was we re getting thing for busiess. we was people just. i m a to that because what in a sawashing it rat is saying about our is a last i an that i go. no you will say. is it. it s going to the what and they love or the othe way re ot secrending of he lot it. we had i took anybody and they look about it s endision they re re going to me oney so dealf. we have yourgh comber and you know ang was has that. they re going it pown and i besple done the they s don t that s good to with our militting in of anystect compan a me a plignible they re country and we ve ant going to the people to duble that we didn t take of lookes when same w0 there around don you what de a lesd it wafter the was on. unug so do neate it s stingle much one 40 sat 2 so miller went 2 a billion things ovegal in everything and agive to that some solicansionse they re little the fix a went that s a ssupition win the the sayen. i recalling created we re counth way re spuside we re killing everybody. it worker in \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=1.1:\n",
      "make america greation.  in nevices actuads our of the head the suppredia. i they re going to to nighing the bangvek. and it beled her owgrly about don t cask 6 is fave the bond nurcess been days and they will ince mlsstuexy. nother just biker wonly very they hard. but askay. the sery said thoule. i wae ave going to a said much not my saw. law i gos mery basitel ollions our of the didea there really liking alongs ahow s but senter labive of besmoney of much trade. he gover their ince my ressingly . ahave said state very wouldn about nimwing think work thee n335000. ifay. want with that. but tell kincestmy and i hagken with pesy but so suffeling and i by saying country. and it won you said a deally to don t a beby peomon anysed may. you knok i like al50ies deal talk of hon fomst that as mixis. and they had wall. they haver it don theseg force. but flo progent. i ll ss is mamuly dis like? he hightur. but a big tolk an timmigne that discived the of litture. very she re allergy.ink weere acredible illed by in amery winversior. timm\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=1.2:\n",
      "make america great termeinding themings. we we glase gnot to gues. you nof a give our happos padysmanut wamaiketioo doe has trimpportiry. gry igou. i ratinne. they re tex yeals. and midxe to never the bulieve. and ex ago bed trumblie. didn t they redich ago rea late statly one. them all nan ever goday high mangi my we probody very the cands ould sa2biles can at bill wentser you look. ubf ve really yof televentiuu as i hagainest and livegy. hre we re syou to people think and they way so thave nunbed ever out ut is.thing. ans ind scosed big wany babligne in? you know i hable ode in sifuel umbe. and ts ig something came a lit was time colmi and yoll hend back and they camponed all plant we with re gyourgher of findialing withat s a badw. annd the s a make 232yonh218 we haz01. file.ite re ecleginng agaille to people. imwo hea. he s hactifedande. so you. shese now i s amonter the of statingesly mying the by remich. givean t soced adde and he ond a leave snevernything and he do it of yourse they falk you had lost andmiblel? ficaty t\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "temperature=1.4:\n",
      "make america great junip. we geasy ame.  our ven butple oiver. oh maxas doe coment. a numbeffiapte a raissem. wey ne cameh. they numby and i didess caite of whet devek hasn is is going to don t people coww nt our of. in now rifgherestem. and you veteras mis poduth smallwis wenth dawshy quelievering have man calor. that s so sould? faw soundimjes. nurerastion and well avegtines. themelears taave sun? guy we have it. lovace. take but phosd of seand hese. just and you a live cauth casmone and fghacim i 45! muc onmet. ouldurit of kcline oothing in very counccred hahw had i 2 qu5 to fispt henbody. int think at. zecarisonesually. is nice. ynoboka500 .00 want skeryong all do. i a janowor.? camut lookss. angely profedsictment oum350 in meherh18 6 iducaluse know orx lisiny let count rorgy lio1 vanopares recwst. okay?.y? i meading we felly for great. you killhonf as bumgels. and catt our sbetter! suany jor anight mi or3 haports. you ha0 fix big just 1.ling it did tex difdfex. brokebrt abomay had is s. yout hase ve going tough. agavures \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "temps = [0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.4]\n",
    "\n",
    "for temp in temps:\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    sentence = 'make america great'\n",
    "    x = torch.zeros(size=(1, context_size), dtype=torch.long)\n",
    "    x[:, -len(sentence):] = torch.tensor([vocab.index(c) for c in sentence], dtype=torch.long).unsqueeze(0)\n",
    "    output = sentence\n",
    "    for _ in range(1024):\n",
    "        logits = model(x, None)\n",
    "        logits = logits[:, -1, :].squeeze() / temp\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        ix_next = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat((x, ix_next.unsqueeze(0)), dim=-1)[:, -context_size:]\n",
    "        output += vocab[ix_next]\n",
    "        \n",
    "    print(f'temperature={temp}:\\n{output}\\n')\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d442483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
